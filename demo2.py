# ===========================================
# ğŸ“„ demo2
# åŠŸèƒ½ï¼šè®€å–å›ºå®šå•é¡Œé›†ï¼Œç”Ÿæˆ Embeddingï¼Œä¸¦ç”¨ Faiss å°æ¯”æ¨¡å‹æ•ˆæœ
# ===========================================

import os
import shutil
import tkinter as tk
from tkinter import filedialog
from collections import defaultdict
from typing import List, Dict

import numpy as np
import faiss
from tqdm import tqdm
from dotenv import load_dotenv
import psycopg2
from psycopg2.extras import RealDictCursor, execute_values

from openai import AzureOpenAI
from FlagEmbedding import BGEM3FlagModel



# ----- ç’°å¢ƒé…ç½® -----
load_dotenv()

print("DEBUG: AOAI_KEY =", os.getenv("AOAI_KEY"))
print("DEBUG: AOAI_ENDPOINT =", os.getenv("AOAI_ENDPOINT"))
print("DEBUG: AOAI_API_VERSION =", os.getenv("AOAI_API_VERSION"))
print("DEBUG: AOAI_CHAT_DEPLOYMENT =", os.getenv("AOAI_CHAT_DEPLOYMENT"))

# åˆå§‹åŒ– AzureOpenAI client
client = AzureOpenAI(
    api_key=os.getenv("AOAI_KEY"),
    azure_endpoint=os.getenv("AOAI_ENDPOINT"),
    api_version=os.getenv("AOAI_API_VERSION")
)

embedding_ada = AzureOpenAI(
    api_key=os.getenv("EMBEDDING_API_KEY"),
    azure_endpoint=os.getenv("EMBEDDING_URL"),
    api_version=os.getenv("AOAI_API_VERSION"),
)

BGE_MODEL = BGEM3FlagModel(
    r"C:\Users\user\.cache\huggingface\hub\models--BAAI--bge-m3\snapshots\5617a9f61b028005a4858fdac845db406aefb181",
    use_fp16=True
)

# DB config
DB_CONFIG = {
    "host": "210.67.12.103",
    "database": "Learning",
    "user": "editor",
    "password": "systex.6214",
    "port": 65432
}

# ----- å·¥å…·å‡½æ•¸ -----
def check_and_clear_table_if_needed():
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        cur.execute('SELECT COUNT(*) FROM public."2500567RAG"')
        existing_count = cur.fetchone()[0]
        
        if existing_count > 0:
            print(f"âš ï¸ è³‡æ–™åº«å·²æœ‰ {existing_count} ç­†è¨˜éŒ„")
            resp = input("è«‹é¸æ“‡å‹•ä½œï¼š\n"
                        "y = åˆªé™¤è³‡æ–™ä¸¦é‡æ–°è™•ç†\n"
                        "n = ä¿ç•™è³‡æ–™ä¸¦ç¹¼çºŒè™•ç†\n"
                        "exit = é›¢é–‹ç¨‹å¼\n"
                        "è¼¸å…¥é¸é … (y/n/exit): ").strip().lower()
            
            if resp.lower() == 'y':
                cur.execute('DELETE FROM public."2500567RAG"')
                cur.execute('TRUNCATE TABLE public."2500567RAG" RESTART IDENTITY;')
                conn.commit()
                print("âœ… å·²æ¸…é™¤è³‡æ–™ï¼Œé‡æ–°é–‹å§‹è™•ç†")
            elif resp == 'n':
                print("âœ… ä¿ç•™ç¾æœ‰è³‡æ–™ï¼Œç¹¼çºŒè™•ç†")
            elif resp == 'exit':
                print("ğŸšª å·²å–æ¶ˆè™•ç†ï¼ŒçµæŸç¨‹å¼")
                cur.close()
                conn.close()
                return False
            else:
                print("âš ï¸ ç„¡æ•ˆé¸é …ï¼Œè«‹é‡æ–°åŸ·è¡Œä¸¦è¼¸å…¥ y/n/exit")
                cur.close()
                conn.close()
                return False
        cur.close()
        conn.close()
        return True
    
    except Exception as e:
        print(f"æª¢æŸ¥/æ¸…é™¤è³‡æ–™å¤±æ•—: {e}")
        return False
    
def wrap_metrics(metrics: dict) -> dict:
    result = {}
    for k, v in metrics.items():
        if k == "Details":
            result[k] = v  # ä¸åŒ…æˆ list
        else:
            result[k] = [v]  # æ­£å¸¸æŒ‡æ¨™åŒ…æˆ list
    return result

    

# ----- embedding å‡½æ•¸ -----

def embed_openai(text: str) -> np.ndarray:
    try:
        response = embedding_ada.embeddings.create(
            input=text,
            model=os.getenv("EMBEDDING_MODEL")
        )
        return np.array(response.data[0].embedding, dtype="float32")
    except Exception as e:
        print(f"embed_openai å‡ºéŒ¯: {e}")
        return np.zeros((1536,), dtype="float32")

def embed_bge(text: str) -> np.ndarray:
    try:
        output = BGE_MODEL.encode(text)
        dense_vecs = output.get("dense_vecs")
        if dense_vecs is None:
            return np.zeros((1024,), dtype="float32")
        return dense_vecs.astype("float32")
    except Exception as e:
        print(f"embed_bge å‡ºéŒ¯: {e}")
        return np.zeros((1024,), dtype="float32")
    
# ----- å»ºç«‹ FAISS ç´¢å¼• -----

def build_faiss_index(vectors: np.ndarray):
    dim = vectors.shape[1]
    faiss.normalize_L2(vectors)
    index = faiss.IndexFlatIP(dim)
    index.add(vectors)
    return index

# ----- è©•ä¼°æŒ‡æ¨™è¨ˆç®— -----

def evaluate_retrieval(index, queries, ground_truth, k=5):
    D, I = index.search(queries, k)
    recall_at_k = []
    precision_at_k = []
    top_k_accuracy = []
    mean_rank = []
    reciprocal_ranks = []
    details = []

    for i, retrieved in enumerate(I):
        relevant = ground_truth[i]
        retrieved_list = list(retrieved)
        detail = {
            "query_id": i,
            "ground_truth": relevant,
            "retrieved_ids": [int(x) for x in retrieved_list],
            "similarities": [float(x) for x in D[i]],
            "hit": False,
            "rank": None,
            "recall": 0,
            "precision": 0,
            "mean_rank": k + 1,
            "reciprocal_rank": 0,
            "top1_hit": False
        }
    
        try:
            rank = retrieved_list.index(relevant)
            recall_at_k.append(1)
            precision_at_k.append(1 / (rank + 1))
            # top_k_accuracy.append(1 if rank == 0 else 0) # Top-1 Accuracy rank == 0 â†’ Top-1 å‘½ä¸­ã€‚
            top_k_accuracy.append(1 if rank < k else 0)    # Top-K Accuracy rank <  k â†’ Top-k å‘½ä¸­
            mean_rank.append(rank + 1)
            reciprocal_ranks.append(1 / (rank + 1))
            detail.update({
                "hit": True,
                "rank": rank + 1,
                "recall": 1,
                "precision": round(1 / (rank + 1), 4),
                "mean_rank": rank + 1,
                "reciprocal_rank": round(1 / (rank + 1), 4),
                "top1_hit": rank == 0
            })
        except ValueError:
            recall_at_k.append(0)
            precision_at_k.append(0)
            top_k_accuracy.append(0)
            mean_rank.append(k + 1)
            reciprocal_ranks.append(0)
            
        details.append(detail)

    return {
        "Recall@K": round(np.mean(recall_at_k), 4),
        "Precision@K": round(np.mean(precision_at_k), 4),
        "Top-K Accuracy": round(np.mean(top_k_accuracy), 4),
        "Mean Rank": round(np.mean(mean_rank), 4),
        "MRR": round(np.mean(reciprocal_ranks), 4),
        "Details": details
    }

# ----- è³‡æ–™å„²å­˜ -----

# def save_evaluation_to_txt(results: dict, run_count: int):
#     filename = f"BGE-OPENAI-embedding test({run_count}).txt"
#     with open(filename, "w", encoding="utf-8") as f:
#         f.write(f"BGE-OPENAI Embedding Evaluation Report (Runs = {run_count})\n")
#         f.write("=" * 60 + "\n\n")
#         for source, models in results.items():
#             f.write(f"ğŸ“„ Source Table: {source}\n")
#             f.write("-" * 60 + "\n")
#             for model_name, metric_data in models.items():
#                 f.write(f"ğŸ”¹ {model_name}:\n")
#                 for metric, values in metric_data.items():
#                     if run_count == 1:
#                         f.write(f"{metric}: {values[0]}\n")
#                     else:
#                         avg = round(sum(values) / len(values), 4)
#                         f.write(f"{metric}: Run1={values[0]}  Run2={values[1]}  Run3={values[2]}  Avg={avg}\n")
#                 f.write("\n")
#     return filename


def prompt_save_as(src_path):
    root = tk.Tk()
    root.withdraw()
    save_path = filedialog.asksaveasfilename(
        title="å¦å­˜çµæœç‚º",
        defaultextension=".txt",
        filetypes=[("Text Files", "*.txt")]
    )
    if save_path:
        shutil.copyfile(src_path, save_path)
        print(f"âœ… æˆåŠŸå¦å­˜ç‚ºï¼š{save_path}")
    else:
        print("âŒ å–æ¶ˆå¦å­˜")
        
def format_details_human_readable(details: List[Dict]) -> str:
    lines = []
    header = f"{'Query':<6}{'GT':<6}{'TopK IDs':<30}{'Rank':<6}{'Top1':<6}{'Recall':<8}{'Prec':<8}{'MRR':<8}"
    lines.append(header)
    lines.append("-" * len(header))
    for d in details:
        line = f"{d['query_id']:<6}{d['ground_truth']:<6}{str(d['retrieved_ids'])[:28]:<30}{str(d['rank'] or '-'):<6}"
        line += f"{'âœ…' if d['top1_hit'] else 'âŒ':<6}{d['recall']:<8}{d['precision']:<8}{d['reciprocal_rank']:<8}"
        lines.append(line)
    return "\n".join(lines)

        
def save_full_report(results_dict, summary_text, run_count):
    import json

    filename = f"å®Œæ•´è©•ä¼°å ±å‘Š_Run{run_count}.txt"
    with open(filename, "w", encoding="utf-8") as f:
        # æ¨™é¡Œ
        f.write(f"[ğŸ“Š] BGE-OPENAI Embedding Evaluation Report (Run {run_count})\n")
        f.write("=" * 80 + "\n\n")

        # æ¯çµ„çµæœ
        for source, models in results_dict.items():
            f.write(f"ğŸ“„ Source Table: {source}\n")
            f.write("-" * 80 + "\n")
            for model_name, metric_data in models.items():
                f.write(f"ğŸ”¹ {model_name} æ¨¡å‹:\n")
                for metric, values in metric_data.items():
                    if run_count == 1:
                        f.write(f"{metric}: {values[0]}\n")
                    else:
                        avg = round(sum(values) / len(values), 4)
                        f.write(f"{metric}: Run1={values[0]}  Run2={values[1]}  Run3={values[2]}  Avg={avg}\n")
                f.write("\n")

        f.write("\n[ğŸ§ ] LLM æ¨¡å‹åˆ†æå ±å‘Š\n")
        f.write("=" * 80 + "\n")
        f.write(summary_text + "\n\n")

        f.write("[ğŸ”] Retrieval è©³ç´°éç¨‹è¨˜éŒ„\n")
        f.write("=" * 80 + "\n")

        for source, models in results_dict.items():
            for model_name, metric_data in models.items():
                if "Details" in metric_data:
                    f.write(f"ğŸ“‚ {source} - {model_name} Retrieval Details:\n")
                    text = format_details_human_readable(metric_data["Details"])
                    f.write(text + "\n\n")               
    return filename

# ----- LLM åˆ†æ -----

def analyze_results_with_llm(results_dict: dict) -> str:
    import json
    text = json.dumps(results_dict, ensure_ascii=False, indent=2)

    analysis_prompt = f"""
ä½ æ˜¯ä¸€å€‹è³‡è¨Šæª¢ç´¢èˆ‡åˆ†æå°ˆå®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æ¨¡å‹å°æ¯”çµæœé€²è¡Œæ·±å…¥åˆ†æã€‚
è«‹ç‰¹åˆ¥é—œæ³¨ Top-1 accuracy èˆ‡ Recall@K æ˜¯å¦å­˜åœ¨æº–ç¢º vs. è¦†è“‹çš„å¹³è¡¡å•é¡Œã€‚

ä½ éœ€è¦æ ¹æ“šä»¥ä¸‹é¢å‘ä¾†ç”¢å‡ºçµæœåˆ†æèˆ‡è¨è«–å®Œæ•´å ±å‘Šï¼š

1. æ•´é«”æº–ç¢ºæ€§æ¯”è¼ƒï¼ˆå“ªå€‹æ¨¡å‹ consistently é ˜å…ˆï¼Ÿï¼‰
2. éŒ¯èª¤æ¡ˆä¾‹å¯èƒ½åŸå› ï¼ˆå“ªäº›æƒ…æ³ä¸‹å…©è€…å·®ç•°å¤§ï¼‰
3. æ€§èƒ½èˆ‡æˆæœ¬æ¯”è¼ƒï¼ˆOpenAI API çš„å»¶é²å’Œè²»ç”¨ vs. æœ¬åœ°éƒ¨ç½²BGEæ¨¡å‹çš„è¨ˆç®—é–‹éŠ·ï¼‰
4. çµè«–ï¼š ç¶œåˆé‡åŒ–æŒ‡æ¨™å’Œè³ªåŒ–åˆ†æï¼Œçµ¦å‡ºæ‡‰ç”¨å»ºè­°ã€‚

è«‹ä»¥æ¸…æ™°æ¢åˆ—æ–¹å¼ç”¢å‡ºåˆ†æå ±å‘Šã€‚ä»¥ä¸‹æ˜¯æŒ‡æ¨™çµæœï¼š
{text}
"""
    response = client.chat.completions.create(
        model=os.getenv("AOAI_CHAT_DEPLOYMENT"),
        messages=[{"role": "user", "content": analysis_prompt}],
        temperature=0.4,
        max_tokens=1000
    )
    return response.choices[0].message.content.strip()


# ----- ä¸»ç¨‹å¼ -----

from collections import defaultdict

def main():
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor(cursor_factory=RealDictCursor)
    cur.execute('SELECT id, question, answer, question_embedding_bge, question_embedding_openai, answer_embedding_bge, answer_embedding_openai, source_table FROM "2500567RAG" ORDER BY id;')
    rows = cur.fetchall()
    conn.close()

    grouped = defaultdict(list)
    for row in rows:
        grouped[row['source_table']].append(row)

    run_count = 1
    all_results = {}
    
    print(f"ğŸ” é–‹å§‹ä¾ç…§ source_table å…± {len(grouped)} çµ„è³‡æ–™é€²è¡Œè©•ä¼°...\n")
    for source, items in tqdm(grouped.items(), desc="è©•ä¼°ä¸­", unit="çµ„"):
        bge_q = np.array([row["question_embedding_bge"] for row in items], dtype='float32')
        bge_a = np.array([row["answer_embedding_bge"] for row in items], dtype='float32')
        openai_q = np.array([row["question_embedding_openai"] for row in items], dtype='float32')
        openai_a = np.array([row["answer_embedding_openai"] for row in items], dtype='float32')
        ground_truth = list(range(len(items)))

        result_bge = evaluate_retrieval(build_faiss_index(bge_a), bge_q, ground_truth)
        result_openai = evaluate_retrieval(build_faiss_index(openai_a), openai_q, ground_truth)

        all_results[source] = {
            "BGE": wrap_metrics(result_bge),
            "OpenAI": wrap_metrics(result_openai)
        }

    # Step 1: ç”¢ç”Ÿåˆ†æå ±å‘Š
    print("\n[ğŸ§ ] LLM åˆ†æå ±å‘Šç”Ÿæˆä¸­...\n")

    # éæ¿¾æ‰ Detailsï¼Œåªä¿ç•™æŒ‡æ¨™æ•¸å€¼
    summary_input = {
        source: {
            model: {k: v for k, v in metrics.items() if k != "Details"}
            for model, metrics in models.items()
        }
        for source, models in all_results.items()
    }

    # ä¸Ÿçµ¦ LLM åˆ†æ
    summary = analyze_results_with_llm(summary_input)
    print(summary)


    # Step 2: æ•´åˆæˆå–®ä¸€å®Œæ•´å ±å‘Š
    full_report_path = save_full_report(all_results, summary, run_count)

    # Step 3: æä¾›ä½¿ç”¨è€…å¦å­˜
    prompt_save_as(full_report_path)



if __name__ == "__main__":
    main()
