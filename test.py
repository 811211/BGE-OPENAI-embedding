"""
GPTâ€‘4o mini vs OSSï¼ˆOllamaï¼‰æ¨¡å‹æ¯”è¼ƒå·¥å…·
-------------------------------------------------
åŠŸèƒ½æ‘˜è¦ï¼š
1) åŒä¸€é¡Œåº«ï¼Œä¸¦è¡Œæ¸¬ GPTâ€‘4o miniï¼ˆAzure OpenAIï¼‰èˆ‡ OSSï¼ˆOllama OpenAIâ€‘compatible APIï¼‰ã€‚
2) è¦ç¯„åŒ– Promptã€è¨˜éŒ„å›æ‡‰ã€é‡æ¸¬å»¶é²ï¼ˆLatencyï¼‰ã€‚
3) æŒ‡æ¨™ï¼šæ­£ç¢ºç‡ã€æ ¼å¼ç¬¦åˆç‡ï¼ˆJSON é¡é¡Œï¼‰ã€å¹³å‡å»¶é²ï¼›ä¿ç•™è³ªåŒ–è§€å¯Ÿæ¬„ä½ã€‚
4) é¡åˆ¥è¦†è“‹ï¼šé‚è¼¯æ¨ç†ã€äº‹å¯¦å•ç­”ã€é•·æ–‡æœ¬ç†è§£ã€å¤šè¼ªå°è©±ã€æ ¼å¼è¼¸å‡ºã€‚
5) ä»¥ .env ç®¡ç†é‡‘é‘°èˆ‡ç«¯é»ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
1) å»ºç«‹è™›æ“¬ç’°å¢ƒä¸¦å®‰è£ä¾è³´ï¼ˆrequestsã€pandasã€python-dotenvã€openaiã€tqdmï¼‰ã€‚
2) æ–¼å°ˆæ¡ˆæ ¹ç›®éŒ„å»ºç«‹ .envï¼ˆè¦‹æœ€ä¸‹æ–¹ç¯„ä¾‹ï¼‰ã€‚
3) ç›´æ¥åŸ·è¡Œï¼špython gpt4o-mini_vs_oss_comparator.py
   ç”¢å‡ºï¼šresults/results.csv èˆ‡ results/summary.csv

æ³¨æ„ï¼š
- å…§å»ºåƒ…æ”¾å°‘é‡ç¤ºä¾‹é¡Œï¼›å¯¦æ¸¬è«‹è‡ªè¡Œæ“´å……ã€Œé¡Œåº«å€ã€æ¸…å–®ï¼ˆå»ºè­°ï¼šé‚è¼¯20ã€äº‹å¯¦20ã€é•·æ–‡10ã€å¤šè¼ª5çµ„ã€æ ¼å¼5ï¼‰ã€‚
- Azure éœ€ä½¿ç”¨ AzureOpenAI å®¢æˆ¶ç«¯ï¼›Ollama ä½¿ç”¨ OpenAI ç›¸å®¹ç«¯é»ï¼ˆ/v1ï¼‰ã€‚
"""

import os
import json
import time
import math
import csv
from pathlib import Path
from typing import List, Dict, Any, Tuple

import pandas as pd
from dotenv import load_dotenv
from openai import AzureOpenAI, OpenAI
from tqdm import tqdm
import requests

# -------------------------------
# 0. è®€å–è¨­å®š
# -------------------------------
load_dotenv()

print("DEBUG: AOAI_KEY =", os.getenv("AOAI_KEY"))
print("DEBUG: AOAI_ENDPOINT =", os.getenv("AOAI_ENDPOINT"))
print("DEBUG: AOAI_API_VERSION =", os.getenv("AOAI_API_VERSION"))
print("DEBUG: AOAI_CHAT_DEPLOYMENT =", os.getenv("AOAI_CHAT_DEPLOYMENT"))
print("DEBUG: OLLAMA_API_BASE =", os.getenv("OLLAMA_API_BASE"))
print("DEBUG: OLLAMA_API_KEY =", os.getenv("OLLAMA_API_KEY"))
print("DEBUG: OLLAMA_MODEL =", os.getenv("OLLAMA_MODEL"))

QUESTIONS_JSON = os.getenv("QUESTIONS_JSON", "questions.json")  

# Azure OpenAIï¼ˆGPTâ€‘4o miniï¼‰
AOAI_KEY = os.getenv("AOAI_KEY", "")
AOAI_ENDPOINT = os.getenv("AOAI_ENDPOINT", "")
AOAI_API_VERSION = os.getenv("AOAI_API_VERSION", "2024-10-21-preview")
AOAI_CHAT_DEPLOYMENT = os.getenv("AOAI_CHAT_DEPLOYMENT", "gpt-4o-mini")  # éƒ¨ç½²åç¨±ï¼ˆç­‰æ–¼ model åƒæ•¸ï¼‰

# Ollamaï¼ˆOpenAIâ€‘compatibleï¼‰
OLLAMA_API_BASE = os.getenv("OLLAMA_API_BASE", "http://127.0.0.1:11434/v1")
OLLAMA_API_KEY = os.getenv("OLLAMA_API_KEY", "ollama")  # ä½”ä½å­—ä¸²å³å¯
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3:8b")

# I/O
RESULT_DIR = Path("results"); RESULT_DIR.mkdir(exist_ok=True)
RESULT_CSV = RESULT_DIR / "results.csv"
SUMMARY_CSV = RESULT_DIR / "summary.csv"

# -------------------------------
# 1. å®¢æˆ¶ç«¯åˆå§‹åŒ–
# -------------------------------
if not AOAI_KEY or not AOAI_ENDPOINT:
    print("âš ï¸  è­¦å‘Šï¼šæœªåµæ¸¬åˆ° AOAI_KEY / AOAI_ENDPOINTï¼ŒAzure ç«¯å¯èƒ½ç„¡æ³•è«‹æ±‚ã€‚")

client_azure = AzureOpenAI(
    api_key=AOAI_KEY,
    azure_endpoint=AOAI_ENDPOINT,
    api_version=AOAI_API_VERSION,
)

client_ollama = OpenAI(
    api_key=OLLAMA_API_KEY,
    base_url=OLLAMA_API_BASE,
)

# -------------------------------
# 2. Prompt æ¨£æ¿
# -------------------------------
PROMPT_REASONING = (
    "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ¨ç†åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æ¢ä»¶å›ç­”å•é¡Œï¼š\n"
    "å•é¡Œï¼š{question}\n"
    "è«‹é€æ­¥æ¨å°ä¸¦å¾—å‡ºæœ€çµ‚ç­”æ¡ˆã€‚"
)

PROMPT_FACT = (
    "ä½ æ˜¯ä¸€å€‹ç²¾æº–çš„çŸ¥è­˜åŠ©æ‰‹ã€‚\n"
    "è«‹å°ä¸‹åˆ—å•é¡Œçµ¦å‡ºç°¡æ½”ä¸”æ˜ç¢ºçš„ç­”æ¡ˆã€‚\n"
    "å•é¡Œï¼š{question}"
)

PROMPT_LONG = (
    "è«‹é–±è®€ä»¥ä¸‹æ–‡ç« ï¼Œå†å›ç­”å•é¡Œã€‚\n\næ–‡ç« ï¼š\n{passage}\n\nå•é¡Œï¼š{question}\n"
    "è«‹å¼•ç”¨æ–‡ä¸­é—œéµå¥ä¸¦ä½œç­”ã€‚"
)

PROMPT_FORMAT_JSON = (
    "è«‹åš´æ ¼ä»¥ JSON è¼¸å‡ºä¸¦ç¬¦åˆéµåèˆ‡å‹åˆ¥ã€‚\n"
    "å•é¡Œï¼š{question}\n"
    "è¼¸å‡ºæ ¼å¼ï¼š{{\"answer\": <å­—ä¸²>, \"confidence\": <0~1æ•¸å­—>}}\n"
    "åªè¼¸å‡º JSONï¼Œä¸è¦å¤šé¤˜æ–‡å­—ã€‚"
)

# å¤šè¼ªå°è©±ï¼šæ¯çµ„æ˜¯ä¸€å€‹ messages list çš„éª¨æ¶ï¼›æœ€å¾ŒåŠ ä¸Š user å•å¥
MULTI_TURN_SYSTEM = {
    "role": "system",
    "content": "ä½ æ˜¯ä¸€ä½è€å¿ƒè€Œåš´è¬¹çš„åŠ©ç†ï¼Œæœƒè¨˜ä½å…ˆå‰è¨­å®šèˆ‡äº‹å¯¦ï¼Œä¸¦åœ¨å¾ŒçºŒå›ç­”ä¸­ç¶­æŒä¸€è‡´ã€‚",
}

# -------------------------------
# 3. é¡Œåº«å€ï¼ˆå®Œæ•´ç‰ˆï¼‰
# -------------------------------

# â¶ é‚è¼¯æ¨ç†ï¼ˆ20 é¡Œï¼‰
logic_questions = [
    {"id": "L1",  "question": "å°æ˜æœ‰ 7 é¡†ç³–ï¼Œé€äº† 3 é¡†ï¼Œåˆè²·äº† 2 é¡†ã€‚ç¾åœ¨æœ‰å¹¾é¡†ï¼Ÿ", "answer": "6"},
    {"id": "L2",  "question": "å¦‚æœä»Šå¤©æ˜¯æ˜ŸæœŸä¸€ï¼Œ100 å¤©å¾Œæ˜¯æ˜ŸæœŸå¹¾ï¼Ÿï¼ˆä»¥ 7 å¤©ç‚ºä¸€é€±ï¼‰", "answer": "æ˜ŸæœŸä¸‰"},
    {"id": "L3",  "question": "ä¸€å€‹é•·æ–¹å½¢é•· 12ã€å¯¬ 5ï¼Œé¢ç©æ˜¯å¤šå°‘ï¼Ÿ", "answer": "60"},
    {"id": "L4",  "question": "æœ€å°çš„æ­£æ•´æ•¸ n èƒ½è¢« 1~10 å…¨æ•´é™¤ï¼Œn æ˜¯å¤šå°‘ï¼Ÿ", "answer": "2520"},
    {"id": "L5",  "question": "ä¸€æ•¸åˆ—ç‚º 1, 1, 2, 3, 5, 8, ä¸‹ä¸€é …æ˜¯å¤šå°‘ï¼Ÿ", "answer": "13"},
    {"id": "L6",  "question": "3x â‰¡ 2 (mod 11)ï¼Œx æœ€å°éè² è§£ï¼Ÿ", "answer": "8"},
    {"id": "L7",  "question": "12 èˆ‡ 18 çš„æœ€å°å…¬å€æ•¸ï¼Ÿ", "answer": "36"},
    {"id": "L8",  "question": "æ™‚é˜ 3:30 æ™‚ï¼Œæ™‚é‡èˆ‡åˆ†é‡å¤¾è§’æœ€å°å€¼ï¼ˆåº¦ï¼‰ï¼Ÿ", "answer": "75"},
    {"id": "L9",  "question": "å¾ 2,6,14,30 ä¾è¦å¾‹çºŒä¸€é …ç‚ºï¼Ÿ", "answer": "62"},  # +4,+8,+16,+32
    {"id": "L10", "question": "ä¾†å›ç­‰è·æ—…ç¨‹ï¼Œå»ç¨‹ 60km/hã€å›ç¨‹ 40km/hï¼Œå¹³å‡é€Ÿç‡ï¼Ÿ", "answer": "48"},
    {"id": "L11", "question": "A å–®ç¨ 6 å°æ™‚å®Œæˆï¼ŒB å–®ç¨ 4 å°æ™‚å®Œæˆï¼Œåˆä½œéœ€å¹¾å°æ™‚ï¼Ÿ", "answer": "2.4"},
    {"id": "L12", "question": "2 çš„ 20 æ¬¡æ–¹é™¤ä»¥ 7 çš„é¤˜æ•¸ï¼Ÿ", "answer": "4"},
    {"id": "L13", "question": "å‰ 50 å€‹å¶æ•¸å’Œï¼ˆ2+4+...+100ï¼‰æ˜¯å¤šå°‘ï¼Ÿ", "answer": "2550"},
    {"id": "L14", "question": "ç”¨ 50ã€10ã€5ã€1 å…ƒæœ€å°‘å¹¾æšæ¹Š 93 å…ƒï¼Ÿ", "answer": "8"},
    {"id": "L15", "question": "ä¸€æ•¸åˆ— 3,7,15,31ï¼Œä¸‹ä¸€é …ï¼Ÿ", "answer": "63"},  # *2+1
    {"id": "L16", "question": "æŸç­ 40 äººï¼Œç”·ç”Ÿ 18ï¼Œå¥³ç”Ÿä½”å…¨ç­å¹¾äººï¼Ÿ", "answer": "22"},
    {"id": "L17", "question": "æœ‰ 24 æ”¯é‰›ç­†ï¼Œå¹³å‡åˆ†çµ¦ 7 äººï¼Œæ¯äººå¹¾æ”¯ã€å‰©å¹¾æ”¯ï¼Ÿï¼ˆæ ¼å¼ï¼ša,bï¼‰", "answer": "3,3"},
    {"id": "L18", "question": "ä¸€æ•¸çš„ 30% æ˜¯ 45ï¼Œè©²æ•¸ç‚ºï¼Ÿ", "answer": "150"},
    {"id": "L19", "question": "å…©æ•¸ç›¸å·® 9ï¼Œå’Œç‚º 41ï¼Œè¼ƒå¤§è€…ç‚ºï¼Ÿ", "answer": "25"},
    {"id": "L20", "question": "ä¸€å•†å“åŸåƒ¹ 500ï¼Œå…ˆæ‰“ 8 æŠ˜å†æ‰“ 9 æŠ˜ï¼Œæœ€çµ‚åƒ¹æ ¼ï¼Ÿ", "answer": "360"},
]

# â· äº‹å¯¦æ€§å•ç­”ï¼ˆ20 é¡Œï¼‰â€” é¿å…æ™‚æ•ˆæ€§ï¼Œé¸ç©©å®šå¸¸è­˜
fact_questions = [
    {"id": "F1",  "question": "æ°´åœ¨æ¨™æº–å¤§æ°£å£“ä¸‹çš„æ²¸é»ï¼ˆÂ°Cï¼‰ï¼Ÿ", "answer": "100"},
    {"id": "F2",  "question": "åœ°çƒæœ‰å¹¾å€‹å¤§æ´²ï¼Ÿ", "answer": "7"},
    {"id": "F3",  "question": "ä¸€åˆ†é˜ç­‰æ–¼å¹¾ç§’ï¼Ÿ", "answer": "60"},
    {"id": "F4",  "question": "Ï€ï¼ˆåœ“å‘¨ç‡ï¼‰ä»¥å…©ä½å°æ•¸è¿‘ä¼¼ç‚ºï¼Ÿ", "answer": "3.14"},
    {"id": "F5",  "question": "äººé¡æœ‰å¹¾å°æŸ“è‰²é«”ï¼Ÿ", "answer": "23"},
    {"id": "F6",  "question": "å…‰é€Ÿç´„ç‚ºæ¯ç§’å¤šå°‘å…¬é‡Œï¼Ÿï¼ˆå–æ•´æ•¸ï¼‰", "answer": "300000"},
    {"id": "F7",  "question": "æ°§æ°£çš„åŒ–å­¸ç¬¦è™Ÿï¼Ÿ", "answer": "O2"},
    {"id": "F8",  "question": "é¹½ï¼ˆé£Ÿé¹½ï¼‰çš„ä¸»è¦æˆåˆ†åŒ–å­¸å¼ï¼Ÿ", "answer": "NaCl"},
    {"id": "F9",  "question": "æ”æ°é›¶åº¦ç›¸ç•¶æ–¼è¯æ°å¹¾åº¦ï¼Ÿï¼ˆå››æ¨äº”å…¥åˆ°æ•´æ•¸ï¼‰", "answer": "32"},
    {"id": "F10", "question": "ä¸€å¹´å¹³å¹´æœ‰å¹¾å¤©ï¼Ÿ", "answer": "365"},
    {"id": "F11", "question": "ä¸–ç•Œä¸Šä½¿ç”¨äººæ•¸æœ€å¤šçš„æ¯èªå¤§è‡´æ˜¯å“ªä¸€èªè¨€å®¶æ—çš„èªè¨€ï¼Ÿï¼ˆç°¡ç­”ï¼‰", "answer": "ä¸­æ–‡"},
    {"id": "F12", "question": "é‡‘çš„åŒ–å­¸ç¬¦è™Ÿï¼Ÿ", "answer": "Au"},
    {"id": "F13", "question": "äººé«”ä¸»è¦å¸å…¥çš„æ°£é«”ä¸­ï¼Œæ°®æ°£ç´„ä½”é«”ç©ç™¾åˆ†æ¯”ï¼Ÿï¼ˆæ•´æ•¸ï¼‰", "answer": "78"},
    {"id": "F14", "question": "DNA çš„å…¨åè‹±æ–‡ç¸®å¯«æ„æŒ‡ï¼Ÿï¼ˆç°¡ç­”ï¼šå¯å¡« â€œDeoxyribonucleic acidâ€ï¼‰", "answer": "Deoxyribonucleic acid"},
    {"id": "F15", "question": "åœ°çƒè‡ªè½‰ä¸€åœˆå¤§ç´„ç‚ºå¹¾å°æ™‚ï¼Ÿ", "answer": "24"},
    {"id": "F16", "question": "å¤ªé™½ç³»ä¸­é«”ç©æœ€å¤§çš„è¡Œæ˜Ÿï¼Ÿ", "answer": "æœ¨æ˜Ÿ"},
    {"id": "F17", "question": "æ”æ°èˆ‡è¯æ°åœ¨ä½•æº«åº¦ç›¸ç­‰ï¼Ÿï¼ˆÂ°Cï¼‰", "answer": "-40"},
    {"id": "F18", "question": "å…¬åˆ¶é•·åº¦å–®ä½ä¸­ï¼Œ1 å…¬å°ºç­‰æ–¼å¹¾å…¬åˆ†ï¼Ÿ", "answer": "100"},
    {"id": "F19", "question": "æµ·å¹³é¢ä¸Šæ¨™æº–å¤§æ°£å£“ç´„ç­‰æ–¼å¹¾ç™¾å¸•ï¼ˆhPaï¼‰ï¼Ÿï¼ˆæ•´æ•¸ï¼‰", "answer": "1013"},
    {"id": "F20", "question": "ç´”æ°´åœ¨ 4Â°C æ™‚çš„å¯†åº¦ï¼ˆg/cm^3ï¼‰ç´„ç‚ºï¼Ÿ", "answer": "1"},
]

# â¸ é•·æ–‡æœ¬ç†è§£ï¼ˆ10 é¡Œï¼‰â€” æ¯é¡Œå«çŸ­æ–‡ + å•é¡Œ + æ‡‰åŒ…å«çš„é—œéµç‰‡èª
long_questions = [
    {
        "id": "T1",
        "passage": "åœ¨è³‡æ–™ç§‘å­¸ä¸­ï¼Œç‰¹å¾µå·¥ç¨‹æ˜¯æå‡æ¨¡å‹è¡¨ç¾çš„é—œéµæ­¥é©Ÿã€‚é€éç¼ºå€¼è£œé½Šã€æ¨™æº–åŒ–èˆ‡é¡åˆ¥ç·¨ç¢¼ï¼Œèƒ½è®“è¨“ç·´æ›´ç©©å®šï¼Œä¸¦ç¸®çŸ­æ”¶æ–‚æ™‚é–“ã€‚",
        "question": "æ–‡ä¸­æåˆ°ç‰¹å¾µå·¥ç¨‹çš„ç›®çš„ç‚ºä½•ï¼Ÿ",
        "answer_contains": "æå‡æ¨¡å‹è¡¨ç¾",
    },
    {
        "id": "T2",
        "passage": "é›²ç«¯é‹ç®—æä¾›å½ˆæ€§çš„é‹ç®—è³‡æºï¼Œä½¿ç”¨è€…å¯ä»¥æŒ‰éœ€æ“´å±•ã€‚é€™é™ä½äº†å‰æœŸç¡¬é«”æŠ•è³‡ï¼Œä¸¦ä¸”è®“æœå‹™æ›´å®¹æ˜“å…¨çƒä½ˆç½²ã€‚",
        "question": "é›²ç«¯é‹ç®—å¸¶ä¾†çš„å…©å€‹ä¸»è¦å„ªå‹¢æ˜¯ä»€éº¼ï¼Ÿ",
        "answer_contains": "å½ˆæ€§çš„é‹ç®—è³‡æº",
    },
    {
        "id": "T3",
        "passage": "ç‰ˆæœ¬æ§åˆ¶ç³»çµ±å¦‚ Gitï¼Œè®“å¤šäººå”ä½œèƒ½å¤ è¿½è¹¤è®Šæ›´ã€åˆ†æ”¯é–‹ç™¼ä¸¦é€²è¡Œåˆä½µã€‚è‰¯å¥½çš„æäº¤è¨Šæ¯æœ‰åŠ©æ–¼å›æº¯å•é¡Œã€‚",
        "question": "ç‰ˆæœ¬æ§åˆ¶ç³»çµ±çš„å…¶ä¸­ä¸€å€‹åŠŸèƒ½æ˜¯å”åŠ©ä»€éº¼ï¼Ÿ",
        "answer_contains": "è¿½è¹¤è®Šæ›´",
    },
    {
        "id": "T4",
        "passage": "åœ¨å°ˆæ¡ˆç®¡ç†ä¸­ï¼Œæ•æ·æ–¹æ³•å¼·èª¿çŸ­é€±æœŸè¿­ä»£ã€æŒçºŒå›é¥‹ï¼Œä»¥åŠè·¨è·èƒ½åœ˜éšŠçš„å¯†åˆ‡åˆä½œï¼Œä»¥å¿«é€Ÿå›æ‡‰éœ€æ±‚è®Šæ›´ã€‚",
        "question": "æ•æ·æ–¹æ³•å¼·èª¿çš„ç¯€å¥ç‚ºä½•ï¼Ÿ",
        "answer_contains": "çŸ­é€±æœŸè¿­ä»£",
    },
    {
        "id": "T5",
        "passage": "è³‡æ–™å¯è¦–åŒ–èƒ½å°‡è¤‡é›œæ•¸æ“šè½‰ç‚ºåœ–å½¢ï¼Œå¹«åŠ©äººå€‘å¿«é€Ÿæ‰¾å‡ºè¶¨å‹¢èˆ‡ç•°å¸¸é»ã€‚é¸æ“‡åˆé©åœ–è¡¨é¡å‹ç›¸ç•¶é‡è¦ã€‚",
        "question": "è³‡æ–™å¯è¦–åŒ–çš„å…¶ä¸­ä¸€å€‹å¥½è™•æ˜¯ä»€éº¼ï¼Ÿ",
        "answer_contains": "å¿«é€Ÿæ‰¾å‡ºè¶¨å‹¢",
    },
    {
        "id": "T6",
        "passage": "åœ¨è»Ÿé«”æ¸¬è©¦ä¸­ï¼Œå–®å…ƒæ¸¬è©¦å°ˆæ³¨æ–¼æœ€å°å¯æ¸¬å–®å…ƒï¼›æ•´åˆæ¸¬è©¦å‰‡é—œæ³¨æ¨¡çµ„ä¹‹é–“çš„äº’å‹•ï¼Œç¢ºä¿ç³»çµ±æ•´é«”è¡Œç‚ºæ­£ç¢ºã€‚",
        "question": "æ•´åˆæ¸¬è©¦é—œæ³¨çš„é‡é»æ˜¯ä»€éº¼ï¼Ÿ",
        "answer_contains": "æ¨¡çµ„ä¹‹é–“çš„äº’å‹•",
    },
    {
        "id": "T7",
        "passage": "è³‡æ–™åº«ç´¢å¼•èƒ½åŠ é€ŸæŸ¥è©¢ï¼Œä½†æœƒå¢åŠ å¯«å…¥æˆæœ¬èˆ‡å„²å­˜ç©ºé–“ã€‚å»ºç«‹ç´¢å¼•å‰æ‡‰è©•ä¼°æŸ¥è©¢æ¨¡å¼èˆ‡æ›´æ–°é »ç‡ã€‚",
        "question": "å»ºç«‹ç´¢å¼•çš„ä»£åƒ¹ä¹‹ä¸€æ˜¯ä»€éº¼ï¼Ÿ",
        "answer_contains": "å¢åŠ å¯«å…¥æˆæœ¬",
    },
    {
        "id": "T8",
        "passage": "æ©Ÿå™¨å­¸ç¿’æ¨¡å‹è‹¥åœ¨è¨“ç·´é›†è¡¨ç¾è‰¯å¥½ä½†åœ¨æ¸¬è©¦é›†è¡¨ç¾å·®ï¼Œç¨±ç‚ºéæ“¬åˆã€‚å¸¸è¦‹ç·©è§£æ–¹æ³•åŒ…æ‹¬æ­£å‰‡åŒ–èˆ‡è³‡æ–™å¢å¼·ã€‚",
        "question": "éæ“¬åˆå¯ç”¨ä½•ç¨®æ–¹æ³•ç·©è§£ï¼Ÿ",
        "answer_contains": "æ­£å‰‡åŒ–",
    },
    {
        "id": "T9",
        "passage": "å¤šåŸ·è¡Œç·’å¯ä»¥æå‡ I/O å¯†é›†å·¥ä½œæ•ˆç‡ï¼Œä½†åœ¨ CPU å¯†é›†å·¥ä½œæ™‚å¯èƒ½å—é™æ–¼ GIL æˆ–ä¸Šä¸‹æ–‡åˆ‡æ›é–‹éŠ·ã€‚",
        "question": "ä½•ç¨®æƒ…å¢ƒä¸‹å¤šåŸ·è¡Œç·’è¼ƒèƒ½æå‡æ•ˆç‡ï¼Ÿ",
        "answer_contains": "I/O å¯†é›†",
    },
    {
        "id": "T10",
        "passage": "å¿«å–æ˜¯ä¸€ç¨®ä»¥ç©ºé–“æ›å–æ™‚é–“çš„ç­–ç•¥ï¼Œå°‡å¸¸ç”¨è³‡æ–™æš«å­˜å¯é™ä½å»¶é²ï¼Œä½†éœ€è¦è¨­è¨ˆå¤±æ•ˆç­–ç•¥ä»¥ç¢ºä¿è³‡æ–™æ–°é®®åº¦ã€‚",
        "question": "ä½¿ç”¨å¿«å–éœ€ç‰¹åˆ¥æ³¨æ„ä½•äº‹é …ï¼Ÿ",
        "answer_contains": "å¤±æ•ˆç­–ç•¥",
    },
]

# â¹ å¤šè¼ªå°è©±ï¼ˆ5 çµ„ï¼‰â€” æ¸¬è¨˜æ†¶èˆ‡ä¸€è‡´æ€§
MULTI_TURN_SYSTEM = {
    "role": "system",
    "content": "ä½ æ˜¯ä¸€ä½è€å¿ƒè€Œåš´è¬¹çš„åŠ©ç†ï¼Œæœƒè¨˜ä½å…ˆå‰è¨­å®šèˆ‡äº‹å¯¦ï¼Œä¸¦åœ¨å¾ŒçºŒå›ç­”ä¸­ç¶­æŒä¸€è‡´ã€‚"
}

multi_turn_sets = [
    {
        "id": "C1",
        "context": [
            MULTI_TURN_SYSTEM,
            {"role": "user", "content": "æˆ‘å«å°è—ï¼Œä¹‹å¾Œè«‹ç”¨æˆ‘çš„åå­—å›è¦†ã€‚"},
            {"role": "assistant", "content": "å¥½çš„ï¼Œå°è—ï¼æˆ‘æœƒè¨˜ä½ä½ çš„åå­—ã€‚"},
        ],
        "follow_up": {"role": "user", "content": "æé†’æˆ‘ä¸€ä¸‹æˆ‘å‰›å‰›èªªæˆ‘å«ä»€éº¼ï¼Ÿ"},
        "expected_contains": "å°è—",
    },
    {
        "id": "C2",
        "context": [
            MULTI_TURN_SYSTEM,
            {"role": "user", "content": "æˆ‘æœ€å–œæ­¡çš„é¡è‰²æ˜¯ç¶ è‰²ï¼Œè«‹è¨˜ä½ã€‚"},
            {"role": "assistant", "content": "æ˜ç™½äº†ï¼Œä½ æœ€å–œæ­¡ç¶ è‰²ã€‚"},
        ],
        "follow_up": {"role": "user", "content": "æˆ‘å‰›å‰›èªªæˆ‘å–œæ­¡å“ªå€‹é¡è‰²ï¼Ÿ"},
        "expected_contains": "ç¶ è‰²",
    },
    {
        "id": "C3",
        "context": [
            MULTI_TURN_SYSTEM,
            {"role": "user", "content": "æˆ‘ä»Šå¤© 15:00 æœ‰æœƒè­°ï¼Œä¹‹å¾Œå•æˆ‘é–‹æœƒæ™‚é–“è¦ä¸€è‡´ã€‚"},
            {"role": "assistant", "content": "æ”¶åˆ°ï¼Œä»Šå¤© 15:00ã€‚"},
        ],
        "follow_up": {"role": "user", "content": "æˆ‘çš„æœƒè­°å¹¾é»ï¼Ÿ"},
        "expected_contains": "15:00",
    },
    {
        "id": "C4",
        "context": [
            MULTI_TURN_SYSTEM,
            {"role": "user", "content": "ä¹‹å¾Œè«‹ç”¨æ•¬èªç¨±å‘¼æˆ‘ç‚ºæ‚¨ã€‚"},
            {"role": "assistant", "content": "å¥½çš„ï¼Œä¹‹å¾Œæˆ‘æœƒä»¥æ‚¨ç¨±å‘¼ã€‚"},
        ],
        "follow_up": {"role": "user", "content": "è«‹å†æé†’æˆ‘ä½ çš„ç¨±å‘¼æ–¹å¼éœ€è¦æ³¨æ„ä»€éº¼ï¼Ÿ"},
        "expected_contains": "æ‚¨",
    },
    {
        "id": "C5",
        "context": [
            MULTI_TURN_SYSTEM,
            {"role": "user", "content": "å‡è¨­å°åŒ—æ˜¯å‡ºç™¼åœ°ï¼Œç›®çš„åœ°æ˜¯å°ä¸­ã€‚"},
            {"role": "assistant", "content": "äº†è§£ï¼Œå¾å°åŒ—å‰å¾€å°ä¸­ã€‚"},
        ],
        "follow_up": {"role": "user", "content": "å‰›å‰›è¨­å®šçš„å‡ºç™¼åœ°æ˜¯å“ªè£¡ï¼Ÿ"},
        "expected_contains": "å°åŒ—",
    },
]

# âº JSON/æ ¼å¼è¼¸å‡ºï¼ˆ5 é¡Œï¼‰â€” åš´æ ¼è¦æ±‚ JSON
json_format_tasks = [
    {"id": "J1", "question": "ç”¨ä¸€å¥è©±è§£é‡‹æ©Ÿå™¨å­¸ç¿’èˆ‡æ·±åº¦å­¸ç¿’çš„å·®ç•°ã€‚"},
    {"id": "J2", "question": "ä»¥ä¸€å¥è©±çµ¦å‡ºè»Ÿé«”æ¸¬è©¦ä¸­å–®å…ƒæ¸¬è©¦çš„å®šç¾©ã€‚"},
    {"id": "J3", "question": "ä»¥ä¸€å¥è©±æè¿°è³‡æ–™åº«ç´¢å¼•çš„å„ªç¼ºé»å„ä¸€ã€‚"},
    {"id": "J4", "question": "ä»¥ä¸€å¥è©±èªªæ˜å¿«å–ï¼ˆcacheï¼‰çš„æ ¸å¿ƒç›®çš„ã€‚"},
    {"id": "J5", "question": "ä»¥ä¸€å¥è©±æè¿°æ•æ·é–‹ç™¼ä¸­è¿­ä»£ï¼ˆiterationï¼‰çš„æ„ç¾©ã€‚"},
]

def load_questions_from_json(path: str) -> bool:
    """
    å¾å¤–éƒ¨ JSON æª”è¼‰å…¥é¡Œåº«ã€‚è‹¥æª”æ¡ˆå­˜åœ¨ä¸”æ ¼å¼æ­£ç¢ºï¼Œå°±è¦†è“‹å…§å»ºæ¸…å–®ï¼›
    è‹¥ä¸å­˜åœ¨æˆ–è®€å–å¤±æ•—ï¼Œç¶­æŒåŸå…§å»ºé¡Œåº«ä¸è®Šã€‚
    JSON çµæ§‹éœ€åŒ…å«ä»»æ„ä¸‹åˆ— keyï¼ˆçš†ç‚º listï¼‰ï¼š
      "logic", "fact", "longtext", "multiturn", "jsonformat"
    """
    p = Path(path)
    if not p.exists():
        print(f"â„¹ï¸  æœªæ‰¾åˆ°å¤–éƒ¨é¡Œåº«ï¼š{path}ï¼Œä½¿ç”¨å…§å»ºé¡Œåº«ã€‚")
        return False
    try:
        with p.open("r", encoding="utf-8") as f:
            data = json.load(f)

        def pick(name, default):
            return data.get(name, default) if isinstance(data.get(name, None), list) else default

        # è¦†è“‹å…§å»ºé¡Œåº«ï¼ˆè‹¥å°æ‡‰ key å­˜åœ¨ï¼‰
        global logic_questions, fact_questions, long_questions, multi_turn_sets, json_format_tasks
        logic_questions      = pick("logic",      logic_questions)
        fact_questions       = pick("fact",       fact_questions)
        long_questions       = pick("longtext",   long_questions)
        multi_turn_sets      = pick("multiturn",  multi_turn_sets)
        json_format_tasks    = pick("jsonformat", json_format_tasks)

        print(f"âœ… å·²è¼‰å…¥å¤–éƒ¨é¡Œåº«ï¼š{path}")
        return True
    except Exception as e:
        print(f"âš ï¸  è¼‰å…¥å¤–éƒ¨é¡Œåº«å¤±æ•—ï¼ˆ{path}ï¼‰ï¼š{e}ï¼Œä½¿ç”¨å…§å»ºé¡Œåº«ã€‚")
        return False

# ç«‹åˆ»å˜—è©¦è¼‰å…¥ï¼ˆè‹¥æª”æ¡ˆä¸å­˜åœ¨å°±å®‰éœä½¿ç”¨å…§å»ºé¡Œåº«ï¼‰
_ = load_questions_from_json(QUESTIONS_JSON)


# -------------------------------
# 4. è«‹æ±‚èˆ‡è©•åˆ†å·¥å…·
# -------------------------------

def ask_azure(messages: List[Dict[str, str]], temperature: float = 0.0) -> Tuple[str, float]:
    """å‘¼å« Azure OpenAIï¼ˆGPTâ€‘4o miniï¼‰ä¸¦å›å‚³ (æ–‡å­—, ç§’æ•¸)ã€‚"""
    t0 = time.perf_counter()
    resp = client_azure.chat.completions.create(
        model=AOAI_CHAT_DEPLOYMENT,  # åœ¨ Azure SDK è£¡ model=éƒ¨ç½²åç¨±
        messages=messages,
        temperature=temperature,
    )
    dt = time.perf_counter() - t0
    content = resp.choices[0].message.content.strip()
    return content, dt


def ask_ollama(messages: List[Dict[str, str]], temperature: float = 0.0) -> Tuple[str, float]:
    """å‘¼å« Ollamaï¼ˆOpenAIâ€‘compatible /v1ï¼‰ä¸¦å›å‚³ (æ–‡å­—, ç§’æ•¸)ã€‚"""
    t0 = time.perf_counter()
    resp = client_ollama.chat.completions.create(
        model=OLLAMA_MODEL,
        messages=messages,
        temperature=temperature,
    )
    dt = time.perf_counter() - t0
    content = resp.choices[0].message.content.strip()
    return content, dt


def is_number_equal(pred: str, gold: str) -> bool:
    """å˜—è©¦å°‡å­—ä¸²ä¸­çš„æ•´æ•¸/æµ®é»æ•¸æŠ“å‡ºä¾†æ¯”å°ï¼›è‹¥é‡‘æ¨™éæ•¸å­—å‰‡é€€å›ç´”å­—ä¸²ç›¸ç­‰ã€‚"""
    def to_num(x: str):
        try:
            return float(x.replace(",", "").strip())
        except:
            return None
    pn, gn = to_num(pred), to_num(gold)
    if pn is not None and gn is not None:
        return math.isclose(pn, gn, rel_tol=1e-6, abs_tol=1e-9)
    return pred.strip() == gold.strip()


def contains_text(pred: str, needle: str) -> bool:
    return needle in pred


def check_json_format(pred: str) -> Tuple[bool, Dict[str, Any]]:
    try:
        obj = json.loads(pred)
        ok = isinstance(obj, dict) and "answer" in obj and "confidence" in obj
        # é€²ä¸€æ­¥æª¢æŸ¥å‹åˆ¥
        if ok and not isinstance(obj.get("answer", None), str):
            ok = False
        if ok:
            conf = obj.get("confidence")
            ok = isinstance(conf, (int, float)) and 0 <= float(conf) <= 1
        return ok, obj if ok else {}
    except Exception:
        return False, {}

# -------------------------------
# 5. åŸ·è¡Œå„é¡Œå‹æ¸¬è©¦
# -------------------------------

def run_logic() -> List[Dict[str, Any]]:
    rows = []
    for item in tqdm(logic_questions, desc="Logic"):
        qid = item["id"]
        question = item["question"]
        gold = item["answer"]
        messages = [{"role": "user", "content": PROMPT_REASONING.format(question=question)}]

        a_txt, a_dt = ask_azure(messages)
        o_txt, o_dt = ask_ollama(messages)

        rows.append({
            "id": qid, "category": "logic", "model": "azure", "latency_s": round(a_dt, 3),
            "answer": a_txt, "correct": int(is_number_equal(a_txt, gold) or contains_text(a_txt, gold)),
            "format_ok": 1, "notes": ""
        })
        rows.append({
            "id": qid, "category": "logic", "model": "oss", "latency_s": round(o_dt, 3),
            "answer": o_txt, "correct": int(is_number_equal(o_txt, gold) or contains_text(o_txt, gold)),
            "format_ok": 1, "notes": ""
        })
    return rows


def run_fact() -> List[Dict[str, Any]]:
    rows = []
    for item in tqdm(fact_questions, desc="Fact"):
        qid = item["id"]
        question = item["question"]
        gold = item["answer"]
        messages = [{"role": "user", "content": PROMPT_FACT.format(question=question)}]

        a_txt, a_dt = ask_azure(messages)
        o_txt, o_dt = ask_ollama(messages)

        rows.append({
            "id": qid, "category": "fact", "model": "azure", "latency_s": round(a_dt, 3),
            "answer": a_txt, "correct": int(is_number_equal(a_txt, gold) or contains_text(a_txt, gold)),
            "format_ok": 1, "notes": ""
        })
        rows.append({
            "id": qid, "category": "fact", "model": "oss", "latency_s": round(o_dt, 3),
            "answer": o_txt, "correct": int(is_number_equal(o_txt, gold) or contains_text(o_txt, gold)),
            "format_ok": 1, "notes": ""
        })
    return rows


def run_long() -> List[Dict[str, Any]]:
    rows = []
    for item in tqdm(long_questions, desc="LongText"):
        qid = item["id"]
        passage = item["passage"]
        question = item["question"]
        needle = item["answer_contains"]
        messages = [{"role": "user", "content": PROMPT_LONG.format(passage=passage, question=question)}]

        a_txt, a_dt = ask_azure(messages)
        o_txt, o_dt = ask_ollama(messages)

        rows.append({
            "id": qid, "category": "longtext", "model": "azure", "latency_s": round(a_dt, 3),
            "answer": a_txt, "correct": int(contains_text(a_txt, needle)),
            "format_ok": 1, "notes": ""
        })
        rows.append({
            "id": qid, "category": "longtext", "model": "oss", "latency_s": round(o_dt, 3),
            "answer": o_txt, "correct": int(contains_text(o_txt, needle)),
            "format_ok": 1, "notes": ""
        })
    return rows


def run_multi_turn() -> List[Dict[str, Any]]:
    rows = []
    for item in tqdm(multi_turn_sets, desc="MultiTurn"):
        qid = item["id"]
        base_ctx = item["context"]
        follow = item["follow_up"]
        expected = item["expected_contains"]

        # Azure
        a_msgs = list(base_ctx) + [follow]
        a_txt, a_dt = ask_azure(a_msgs)

        # OSS
        o_msgs = list(base_ctx) + [follow]
        o_txt, o_dt = ask_ollama(o_msgs)

        rows.append({
            "id": qid, "category": "multiturn", "model": "azure", "latency_s": round(a_dt, 3),
            "answer": a_txt, "correct": int(contains_text(a_txt, expected)),
            "format_ok": 1, "notes": ""
        })
        rows.append({
            "id": qid, "category": "multiturn", "model": "oss", "latency_s": round(o_dt, 3),
            "answer": o_txt, "correct": int(contains_text(o_txt, expected)),
            "format_ok": 1, "notes": ""
        })
    return rows


def run_json_tasks() -> List[Dict[str, Any]]:
    rows = []
    for item in tqdm(json_format_tasks, desc="JSONFormat"):
        qid = item["id"]
        question = item["question"]
        messages = [{"role": "user", "content": PROMPT_FORMAT_JSON.format(question=question)}]

        a_txt, a_dt = ask_azure(messages)
        a_ok, _ = check_json_format(a_txt)
        o_txt, o_dt = ask_ollama(messages)
        o_ok, _ = check_json_format(o_txt)

        rows.append({
            "id": qid, "category": "json", "model": "azure", "latency_s": round(a_dt, 3),
            "answer": a_txt, "correct": int(a_ok), "format_ok": int(a_ok), "notes": ""
        })
        rows.append({
            "id": qid, "category": "json", "model": "oss", "latency_s": round(o_dt, 3),
            "answer": o_txt, "correct": int(o_ok), "format_ok": int(o_ok), "notes": ""
        })
    return rows

# -------------------------------
# 6. åŒ¯ç¸½èˆ‡è¼¸å‡º
# -------------------------------

def summarize_and_save(rows: List[Dict[str, Any]]) -> None:
    df = pd.DataFrame(rows)
    df.to_csv(RESULT_CSV, index=False, encoding="utf-8-sig")

    # ä¾ model+category èšåˆ
    grp = df.groupby(["model", "category"]).agg(
        avg_latency_s=("latency_s", "mean"),
        accuracy=("correct", "mean"),
        format_ok_rate=("format_ok", "mean"),
        n=("id", "count")
    ).reset_index()

    # è½‰ç™¾åˆ†æ¯”èˆ‡å››æ¨äº”å…¥
    grp["accuracy"] = (grp["accuracy"] * 100).round(1)
    grp["format_ok_rate"] = (grp["format_ok_rate"] * 100).round(1)
    grp["avg_latency_s"] = grp["avg_latency_s"].round(3)

    grp.to_csv(SUMMARY_CSV, index=False, encoding="utf-8-sig")

    print("\n===== Summary =====")
    print(grp.to_string(index=False))
    print(f"\nğŸ“„ æ˜ç´°ï¼š{RESULT_CSV}")
    print(f"ğŸ“Š æ‘˜è¦ï¼š{SUMMARY_CSV}")

# -------------------------------
# 7. ä¸»ç¨‹å¼
# -------------------------------

def main():
    all_rows: List[Dict[str, Any]] = []

    # ä»»å‹™è¡¨ç¾é¢
    all_rows += run_logic()
    all_rows += run_fact()
    all_rows += run_long()

    # ä½¿ç”¨è€…é«”é©—é¢ï¼ˆå¤šè¼ªä¸€è‡´æ€§ï¼‰
    all_rows += run_multi_turn()

    # æŒ‡ä»¤éµå®ˆ / æ ¼å¼è¼¸å‡º
    all_rows += run_json_tasks()

    summarize_and_save(all_rows)

if __name__ == "__main__":
    main()

"""
.env ç¯„ä¾‹ï¼š
--------------------------------
# Azure OpenAI
AOAI_KEY=ä½ çš„_AzureOpenAI_Key
AOAI_ENDPOINT=https://ä½ çš„è³‡æºåç¨±.openai.azure.com/
AOAI_API_VERSION=2024-10-21-preview
AOAI_DEPLOYMENT=gpt-4o-mini   # ä½ çš„éƒ¨ç½²åç¨±

# Ollama OpenAIâ€‘compatible
OLLAMA_API_BASE=http://127.0.0.1:11434/v1
OLLAMA_API_KEY=ollama
OLLAMA_MODEL=llama3:8b
--------------------------------
æ“´å……é¡Œåº«ï¼š
- å°‡ logic_questions / fact_questions / long_questions / multi_turn_sets / json_format_tasks ä¾å»ºè­°æ•¸é‡æ“´å……ã€‚
- è‹¥è¦å¤–éƒ¨æª”æ¡ˆç®¡ç†ï¼Œå¯è‡ªè¨‚ loaderï¼ˆè®€å– JSON/CSVï¼‰å¾Œå¡«å……ä¸Šè¿°æ¸…å–®ã€‚
"""
