# ===========================================
# ğŸ“„ test
# åŠŸèƒ½ï¼šè‡¨æ™‚æ¸¬è©¦ç”¨
# ===========================================
import psycopg2
import pandas as pd
import json
import re
from typing import List, Dict, Union  # Added Union for type hinting flexibility
import requests
import os
import threading
from queue import Queue, Empty
from io import StringIO
from openai import AzureOpenAI
from dotenv import load_dotenv
import os
from typing import List
import torch
from FlagEmbedding import BGEM3FlagModel  # Correct import for BGEM3FlagModel

load_dotenv()
# --- è³‡æ–™åº«é€£ç·šé…ç½® ---
# !!! è«‹æ›¿æ›ç‚ºä½ å€‘å…¬å¸çš„å¯¦éš› PSQL é€£ç·šè³‡è¨Š !!!
DB_CONFIG = {
    "host": "20.210.159.117",
    "database": "postgres",
    "user": "readonlyuser",
    "password": "systex.6214",
    "port": "65432"  
}


def fetch_data_from_psql() -> List[Dict]:
    """
    å¾ PSQL è³‡æ–™åº«çš„æŒ‡å®š Table æå–æ•¸æ“šä¸¦é€²è¡Œåˆæ­¥è™•ç†ã€‚
    å°‡ headline å’Œ story çµåˆæˆ full_textã€‚
    """
    conn = None
    all_processed_docs = []

    try:
        print("å˜—è©¦é€£æ¥ PostgreSQL è³‡æ–™åº«...")
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        print("æˆåŠŸé€£æ¥åˆ° PostgreSQL è³‡æ–™åº«ã€‚")

        # --- 1. è™•ç†æ–°èé¡çš„ Table: ntn001rtnews, ntn002rtnews, ntnx1netrtnews ---
        news_tables = ["ntn001rtnews", "ntn002rtnews", "ntnx1netrtnews"]
        # !!! é‡è¦ï¼šè«‹æ›¿æ›ç‚ºä½ å€‘æ–°è Table ä¸­å¯¦éš›çš„ ID æ¬„ä½åç¨±å’Œæ—¥æœŸ/æ™‚é–“æ¬„ä½åç¨± !!!
        # å‡è¨­æ–°è Table æœ‰ä¸€å€‹å”¯ä¸€çš„ ID æ¬„ä½å« 'article_id'
        # å‡è¨­æœ‰ä¸€å€‹ç™¼å¸ƒæ™‚é–“æ¬„ä½å« 'publish_time'
        NEWS_ID_COLUMN = "id"  # è«‹æ›¿æ›

        for table_name in news_tables:
            print(f"\næ­£åœ¨å¾ Table '{table_name}' æå–æ•¸æ“š...")
            # é€™è£¡æˆ‘ç”¨ LIMIT 1000 é™åˆ¶æ•¸æ“šé‡ï¼ŒåˆæœŸæ¸¬è©¦å¯ä»¥é€™æ¨£åšã€‚
            # æ­£å¼æŠ“å–æ™‚ï¼Œå¯èƒ½éœ€è¦èª¿æ•´ç‚ºæ²’æœ‰ LIMIT æˆ–æ ¹æ“šæ™‚é–“ç¯„åœä¾†æŠ“å–ã€‚
            query = f"""
            SELECT "{NEWS_ID_COLUMN}", headline, story
            FROM {table_name}
            LIMIT 100;
            """

            try:
                cursor.execute(query)
                rows = cursor.fetchall()
                column_names = [desc[0] for desc in cursor.description]  # ç²å–æ¬„ä½åç¨±

                for row in rows:
                    row_dict = dict(zip(column_names, row))

                    doc_id_raw = str(row_dict[NEWS_ID_COLUMN])
                    headline = (
                        row_dict.get("headline", "")
                        if row_dict.get("headline") is not None
                        else ""
                    )
                    story = (
                        row_dict.get("story", "")
                        if row_dict.get("story") is not None
                        else ""
                    )

                    # çµåˆ headline å’Œ story
                    # å¦‚æœ headline å’Œ story éƒ½æœ‰ï¼Œå‰‡ç”¨æ›è¡Œç¬¦éš”é–‹ï¼›å¦å‰‡åªå–éç©ºçš„é‚£å€‹
                    full_text = (
                        f"{headline}\n\n{story}"
                        if headline and story
                        else (headline if headline else story)
                    )

                    if full_text.strip():  # ç¢ºä¿æ–‡æœ¬éç©º
                        all_processed_docs.append(
                            {
                                "original_id": f"{table_name}_{doc_id_raw}",  # å‰µå»ºä¸€å€‹å…¨å±€å”¯ä¸€çš„ID
                                "raw_text": full_text,
                                "source_table": table_name,
                                "type": "news",
                                "title": headline,  # å°‡ headline ä¹Ÿä½œç‚º title å…ƒæ•¸æ“š
                                "publish_time": str(
                                    row_dict.get("datetime", "")
                                ),  # å°‡æ™‚é–“è½‰æ›ç‚ºå­—ä¸²
                            }
                        )
                print(f"æˆåŠŸå¾ '{table_name}' æå– {len(rows)} æ¢è¨˜éŒ„ã€‚")
            except psycopg2.Error as e_inner:
                print(f"å¾ Table '{table_name}' æå–æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e_inner}")
                # å¯ä»¥é¸æ“‡è·³éç•¶å‰ Table ç¹¼çºŒè™•ç†ä¸‹ä¸€å€‹

        # --- 2. è™•ç†ç ”ç©¶å ±å‘Š Table: rshcontent ---
        print("\næ­£åœ¨å¾ Table 'rshcontent' æå–æ•¸æ“š...")
        # !!! é‡è¦ï¼šè«‹æ›¿æ›ç‚ºä½ å€‘ç ”ç©¶å ±å‘Š Table ä¸­å¯¦éš›çš„ ID æ¬„ä½åç¨± !!!
        RSH_ID_COLUMN = "reportid"  # è«‹æ›¿æ›

        query_rsh = f"""
        SELECT "{RSH_ID_COLUMN}", content
        FROM rshcontent
        LIMIT 100;
        """

        try:
            cursor.execute(query_rsh)
            rows_rsh = cursor.fetchall()
            column_names_rsh = [desc[0] for desc in cursor.description]

            for row_rsh in rows_rsh:
                row_dict_rsh = dict(zip(column_names_rsh, row_rsh))

                doc_id_raw_rsh = str(row_dict_rsh[RSH_ID_COLUMN])
                content = (
                    row_dict_rsh.get("content", "")
                    if row_dict_rsh.get("content") is not None
                    else ""
                )

                if content.strip():
                    all_processed_docs.append(
                        {
                            "original_id": f"rshcontent_{doc_id_raw_rsh}",
                            "raw_text": content,
                            "source_table": "rshcontent",
                            "type": "research_report",
                        }
                    )
            print(f"æˆåŠŸå¾ 'rshcontent' æå– {len(rows_rsh)} æ¢è¨˜éŒ„ã€‚")
        except psycopg2.Error as e_inner:
            print(f"å¾ Table 'rshcontent' æå–æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e_inner}")

    except psycopg2.Error as ex:
        print(f"è³‡æ–™åº«é€£æ¥æˆ–æ“ä½œç™¼ç”ŸéŒ¯èª¤: {ex}")
    finally:
        if conn:
            conn.close()
            print("\nPostgreSQL é€£æ¥å·²é—œé–‰ã€‚")

    return all_processed_docs


def query_aoai_embedding(content: str) -> list[float]:
    """å¾ Azure OpenAI æœå‹™ç²å–æ–‡æœ¬çš„ embedding å‘é‡

    Args:
        content (str): è¦é€²è¡Œ embedding çš„æ–‡æœ¬å…§å®¹

    Returns:
        list[float]: è¿”å› embedding å‘é‡ï¼Œå¦‚æœç™¼ç”ŸéŒ¯èª¤å‰‡è¿”å›ç©ºåˆ—è¡¨
    """
    try_cnt = 2
    while try_cnt > 0:
        try_cnt -= 1
        api_key = os.getenv("EMBEDDING_API_KEY")
        api_base = os.getenv("EMBEDDING_URL")
        embedding_model = os.getenv("EMBEDDING_MODEL")
        print("load successfully embedding model:", embedding_model)
        try:
            client = AzureOpenAI(
                api_key=api_key,
                azure_endpoint=api_base,
            )
            embedding = client.embeddings.create(
                input=content,
                model=embedding_model,
            )
            return embedding.data[0].embedding
        except Exception as e:
            print(f"get_embedding_resource error | err_msg={e}")
    return []


import os
import threading
from typing import List, Dict, Union
import torch
from FlagEmbedding import BGEM3FlagModel  # Make sure this is imported

# Global model instance for BGE to avoid re-loading
bge_m3_model_instance = None
bge_model_lock = (
    threading.Lock()
)  # To ensure thread-safe loading if used in multi-thread


def load_bge_m3_model():
    """Loads the BGEM3FlagModel once."""
    global bge_m3_model_instance
    with bge_model_lock:
        if bge_m3_model_instance is None:
            try:
                print(
                    "Loading BGE-M3 model (FlagEmbedding)... This might take a moment if not cached."
                )
                # use_fp16=True can save memory and speed up on compatible GPUs,
                # but might cause issues on some CPU setups or older GPUs.
                bge_m3_model_instance = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True)
                print("BGE-M3 model loaded successfully.")
            except Exception as e:
                print(f"Error loading BGE-M3 model: {e}")
                bge_m3_model_instance = None
        return bge_m3_model_instance


# Corrected function signature for query_bge_embedding
def query_bge_embedding(
    content: Union[str, List[str]],
) -> Union[List[float], List[List[float]]]:
    """
    å¾æœ¬åœ°è¼‰å…¥çš„ BGE-M3 æ¨¡å‹ç²å–æ–‡æœ¬çš„ embedding å‘é‡ã€‚
    æ”¯æ´å–®å€‹å­—ä¸²æˆ–å­—ä¸²åˆ—è¡¨è¼¸å…¥ã€‚

    Args:
        content (str | List[str]): è¦é€²è¡Œ embedding çš„æ–‡æœ¬å…§å®¹æˆ–æ–‡æœ¬åˆ—è¡¨ã€‚

    Returns:
        List[float] (å–®å€‹æ–‡æœ¬) æˆ– List[List[float]] (æ–‡æœ¬åˆ—è¡¨): è¿”å› embedding å‘é‡ã€‚
    """
    model = load_bge_m3_model()
    if model is None:
        print("BGE-M3 model is not loaded. Cannot generate embedding.")
        return []

    try:
        # Based on BAAI example, access 'dense_vecs' key
        embeddings_output = model.encode(content)  # This returns a dictionary

        # Access the dense vectors from the dictionary
        dense_embeddings = embeddings_output["dense_vecs"]

        if isinstance(content, str):
            # If input was a single string, 'dense_vecs' will contain a single embedding array.
            # dense_embeddings will be a NumPy array of shape (embedding_dim,) if only one sentence.
            # If batch_size=1, it might still return (1, embedding_dim), so we need to handle that.
            # The BAAI example implies it will be a 2D array even for single input if batch_size is > 1
            # but if you pass a single string, it's typically (embedding_dim,).
            # To be safe, let's assume it always returns a 2D array like (num_sentences, embedding_dim)
            # and take the first (and only) embedding.
            if dense_embeddings.ndim == 2 and dense_embeddings.shape[0] == 1:
                return dense_embeddings[0].tolist()
            else:  # If it somehow returns 1D for single string
                return dense_embeddings.tolist()
        else:
            # If input was a list of strings, dense_embeddings will be a NumPy array of arrays
            return dense_embeddings.tolist()

    except Exception as e:
        print(f"query_bge_embedding error | err_msg={e}")
        return []


# --- æ•´åˆå¾ŒçºŒçš„æ¸…æ´—ã€åˆ†å¡Šã€ä¿å­˜åˆ° JSONL æª”æ¡ˆçš„æµç¨‹ ---
# ï¼ˆé€™éƒ¨åˆ†èˆ‡ä¹‹å‰æä¾›çš„ç¨‹å¼ç¢¼ç›¸åŒï¼Œåªæ˜¯ç¾åœ¨ raw_extracted_data æœƒä¾†è‡ª fetch_data_from_psql()ï¼‰

# 1. åŸ·è¡Œæ•¸æ“šæå–
raw_extracted_data = fetch_data_from_psql()
print(f"\nç¸½å…±å¾è³‡æ–™åº«æå–åˆ° {len(raw_extracted_data)} æ¢åŸå§‹è¨˜éŒ„ã€‚")

print(query_bge_embedding("é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡æœ¬ã€‚"))
print(query_aoai_embedding("é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡æœ¬ã€‚"))

# 2. æ¸…æ´—èˆ‡åˆ†å¡Š (å‡è¨­ process_single_document å‡½æ•¸å·²å®šç¾©)
# processed_docs_for_chunking = []
# for doc_item in raw_extracted_data:
#     processed_chunks = process_single_document(doc_item)
#     processed_docs_for_chunking.extend(processed_chunks)

# 3. ä¿å­˜åˆ° JSONL æª”æ¡ˆ (å‡è¨­ save_to_jsonl å‡½æ•¸å·²å®šç¾©)
# output_cleaned_chunks_file = 'company_financial_corpus_chunks.jsonl'
# with open(output_cleaned_chunks_file, 'w', encoding='utf-8') as f:
#     for item in processed_docs_for_chunking:
#         f.write(json.dumps(item, ensure_ascii=False) + '\n')
# print(f"\næ‰€æœ‰æ¸…æ´—åˆ†å¡Šå¾Œçš„æ•¸æ“šå·²ä¿å­˜åˆ° '{output_cleaned_chunks_file}'ï¼Œå…± {len(processed_docs_for_chunking)} å€‹å¡Šã€‚")
