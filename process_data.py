import os
import re
import hashlib
from datetime import datetime
from typing import List, Dict, Tuple
import psycopg2
from psycopg2.extras import execute_values
import numpy as np
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from utils.database_config import get_database_config
from utils.ai_client import get_embedding_for_content

DB_CONFIG = {
    "host": "210.67.12.103",
    "database": "Learning",
    "user": "editor",
    "password": "systex.6214",
    "port": "65432"
}

class WenWangQianProcessor:
    """æ–‡ç‹ç±¤è™•ç†å™¨"""

    def __init__(self):
        self.db_config = {
            "host": "210.67.12.103",
            "database": "Learning",
            "user": "editor",
            "password": "systex.6214",
            "port": 65432  
        }
        self.max_workers = 4
        self.lock = threading.Lock()

    def read_text(self, file_path: str) -> List[Dict]:
        """è®€å–æ–‡ç‹ç±¤.txt ä¸¦æŒ‰ç±¤è™Ÿåˆ†å‰²"""
        print(f"ğŸ“– æ­£åœ¨è®€å–æ–‡ç‹ç±¤è³‡æ–™: {file_path}")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            # ä½¿ç”¨æ­£å‰‡åˆ†æ®µ
            pattern = r'(\d+ï¼š.*?)(?=\n\d+ï¼š|\Z)'  # æ•æ‰ä¸€ç±¤åˆ°ä¸‹ä¸€ç±¤ä¹‹é–“çš„æ–‡å­—
            matches = re.findall(pattern, text, re.DOTALL)
            results = []
            for match in matches:
                m = re.match(r'(\d+)ï¼š(.*)', match, re.DOTALL)
                if m:
                    num = m.group(1)
                    content = m.group(2).strip().replace('\n', ' ')
                    context = f"æ–‡ç‹ç±¤ ç±¤è™Ÿ {num} "
                    results.append({
                        'content': content,
                        'context': context,
                        'chunk_index': int(num)
                    })
            print(f"âœ… è®€å–å®Œæˆï¼Œå…± {len(results)} é¦–ç±¤è©©")
            return results
        except Exception as e:
            print(f"âŒ è®€å–æ–‡ç‹ç±¤.txt å¤±æ•—: {e}")
            return []

    def query_aoai_embedding(self, content: str) -> list[float]:
        return get_embedding_for_content(content)

    def process_embedding_batch(self, chunks_batch: List[Dict], batch_id: int, total_batches: int) -> List[Dict]:
        processed_chunks = []
        for i, chunk in enumerate(chunks_batch):
            try:
                embedding = self.query_aoai_embedding(chunk['content'])
                chunk['embedding'] = embedding
                with self.lock:
                    print(f"ğŸ§µ åŸ·è¡Œç·’ {batch_id}/{total_batches} | ç±¤ {chunk['chunk_index']}")
                processed_chunks.append(chunk)
                time.sleep(0.1)
            except Exception as e:
                with self.lock:
                    print(f"âŒ åŸ·è¡Œç·’ {batch_id} ç™¼ç”ŸéŒ¯èª¤: {e}")
                chunk['embedding'] = None
                processed_chunks.append(chunk)
        return processed_chunks

    def generate_embeddings(self, chunks: List[Dict]) -> List[Dict]:
        print(f"ğŸš€ é–‹å§‹ç”Ÿæˆ embeddingï¼Œå…± {len(chunks)} é¦–ç±¤è©©")
        batch_size = len(chunks) // self.max_workers + 1
        batches = [chunks[i:i + batch_size] for i in range(0, len(chunks), batch_size)]
        all_processed_chunks = []

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self.process_embedding_batch, b, i+1, len(batches)): i for i, b in enumerate(batches)}
            for future in as_completed(futures):
                all_processed_chunks.extend(future.result())

        all_processed_chunks.sort(key=lambda x: x['chunk_index'])
        print(f"âœ… embedding ç”Ÿæˆå®Œæˆï¼Œå…± {len(all_processed_chunks)} æ¢")
        return all_processed_chunks

    def create_database_tables(self):
        print("æ­£åœ¨å‰µå»ºè³‡æ–™åº«è¡¨æ ¼...")
        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()
            cur.execute("""
                CREATE TABLE IF NOT EXISTS public.\"2500567RAG\"
                (
                    id SERIAL NOT NULL,
                    embedding_vector double precision[],
                    created_at timestamp without time zone DEFAULT CURRENT_TIMESTAMP,
                    content text COLLATE pg_catalog."default",
                    context text NOT NULL,
                    CONSTRAINT embeddings_pkey PRIMARY KEY (id)
                );
            """)
            cur.execute("""
                CREATE INDEX IF NOT EXISTS idx_embeddings_content ON \"2500567RAG\" USING gin(to_tsvector('chinese', content));
                CREATE INDEX IF NOT EXISTS idx_embeddings_context ON \"2500567RAG\"(context);
            """)
            conn.commit()
            cur.close()
            conn.close()
            print("âœ… è³‡æ–™åº«è¡¨æ ¼å°±ç·’")
        except Exception as e:
            print(f"âŒ å‰µå»ºè³‡æ–™åº«æ™‚éŒ¯èª¤: {e}")

    def save_to_database(self, chunks: List[Dict]):
        print("ğŸ’¾ å„²å­˜åˆ°è³‡æ–™åº«...")
        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()
            embedding_data = [(c['embedding'], c['content'], c['context']) for c in chunks if c.get('embedding')]
            execute_values(cur, """
                INSERT INTO public.\"2500567RAG\" (embedding_vector, content, context) VALUES %s
            """, embedding_data)
            conn.commit()
            cur.close()
            conn.close()
            print(f"âœ… å„²å­˜å®Œæˆï¼Œå…± {len(embedding_data)} ç­†è¨˜éŒ„")
        except Exception as e:
            print(f"âŒ å„²å­˜è³‡æ–™åº«æ™‚éŒ¯èª¤: {e}")

    def check_existing_data(self) -> int:
        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()
            cur.execute("SELECT COUNT(*) FROM public.\"2500567RAG\"")
            count = cur.fetchone()[0]
            cur.close()
            conn.close()
            return count
        except Exception as e:
            print(f"æª¢æŸ¥è³‡æ–™åº«éŒ¯èª¤: {e}")
            return 0

    def process_text(self, file_path: str):
        print("=" * 50)
        print("é–‹å§‹è™•ç†æ–‡ç‹ç±¤è³‡æ–™")
        print("=" * 50)

        self.create_database_tables()
        existing_count = self.check_existing_data()
        if existing_count > 0:
            print(f"âš ï¸ è³‡æ–™åº«å·²æœ‰ {existing_count} ç­†è¨˜éŒ„")
            resp = input("æ¸…é™¤ç¾æœ‰è³‡æ–™ä¸¦é‡æ–°è™•ç†ï¼Ÿ(y/N): ")
            if resp.lower() == 'y':
                try:
                    conn = psycopg2.connect(**self.db_config)
                    cur = conn.cursor()
                    cur.execute("DELETE FROM public.\"2500567RAG\"")
                    conn.commit()
                    cur.close()
                    conn.close()
                    print("âœ… å·²æ¸…é™¤è³‡æ–™")
                except Exception as e:
                    print(f"æ¸…é™¤è³‡æ–™å¤±æ•—: {e}")
                    return
            else:
                print("å–æ¶ˆè™•ç†")
                return

        chunks = self.read_text(file_path)
        if not chunks:
            print("âŒ è®€å–å¤±æ•—ï¼Œä¸­æ­¢åŸ·è¡Œ")
            return

        chunks_emb = self.generate_embeddings(chunks)
        self.save_to_database(chunks_emb)
        print("=" * 50)
        print("ğŸ“¦ æ–‡ç‹ç±¤è™•ç†å®Œæˆ")
        print("=" * 50)
        final_count = self.check_existing_data()
        print(f"ğŸ“Š ç¾åœ¨è³‡æ–™åº«ä¸­æœ‰ {final_count} ç­†è¨˜éŒ„")

def main():
    processor = WenWangQianProcessor()
    file_path = "æ–‡ç‹ç±¤.txt"
    if not os.path.exists(file_path):
        print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_path}")
        return
    processor.process_text(file_path)

if __name__ == "__main__":
    main()