# ===========================================
# ğŸ“„ QA_Embedding
# åŠŸèƒ½ï¼šç”Ÿæˆå•é¡Œä¸¦æ¯”è¼ƒæ¨¡å‹
# ===========================================
import json
import os
import psycopg2
import numpy as np
import faiss
from tqdm import tqdm
from typing import List, Dict
from openai import AzureOpenAI
from dotenv import load_dotenv
from psycopg2.extras import execute_values
from FlagEmbedding import BGEM3FlagModel

# ----- ç’°å¢ƒé…ç½® -----
load_dotenv()

print("DEBUG: AOAI_KEY =", os.getenv("AOAI_KEY"))
print("DEBUG: AOAI_ENDPOINT =", os.getenv("AOAI_ENDPOINT"))
print("DEBUG: AOAI_API_VERSION =", os.getenv("AOAI_API_VERSION"))
print("DEBUG: AOAI_CHAT_DEPLOYMENT =", os.getenv("AOAI_CHAT_DEPLOYMENT"))

# åˆå§‹åŒ– AzureOpenAI client
client = AzureOpenAI(
    api_key=os.getenv("AOAI_KEY"),
    azure_endpoint=os.getenv("AOAI_ENDPOINT"),
    api_version=os.getenv("AOAI_API_VERSION")
)

embedding_ada = AzureOpenAI(
    api_key=os.getenv("EMBEDDING_API_KEY"),
    azure_endpoint=os.getenv("EMBEDDING_URL"),
    api_version=os.getenv("AOAI_API_VERSION"),
)

BGE_MODEL = BGEM3FlagModel(
    r"C:\Users\user\.cache\huggingface\hub\models--BAAI--bge-m3\snapshots\5617a9f61b028005a4858fdac845db406aefb181",
    use_fp16=True
)

# DB config
DB_CONFIG = {
    "host": "210.67.12.103",
    "database": "Learning",
    "user": "editor",
    "password": "systex.6214",
    "port": 65432
}

# ----- æŠ½å–è³‡æ–™åº«æ–‡ä»¶ -----
def fetch_documents(limit=1000) -> List[Dict]:
    max_chars = 2000
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    
    cur.execute('SELECT DISTINCT source_table FROM "2500567RAG2";')
    source_types = [row[0] for row in cur.fetchall()]
    
    result = {}

    for s_type in source_types:
        cur.execute('SELECT document_id, text FROM "2500567RAG2" WHERE source_table = %s LIMIT %s;', (s_type, limit))
        docs = []
        for row in cur.fetchall():
            doc_id, text = row
            if not isinstance(text, str):
                text = str(text)
            text = text.strip()[:max_chars]
            docs.append({"document_id": doc_id, "text": text, "source_table": s_type})
        result[s_type] = docs
    
    conn.close()
    return result

# ----- ç”Ÿæˆå‡å•é¡Œ -----
def generate_questions_for_docs(docs: List[Dict], total_questions=100) -> List[Dict]:
    chat_deployment = os.getenv("AOAI_CHAT_DEPLOYMENT")
    questions = []
    existing_questions = set()  # â¤ ç”¨æ–¼é¿å…é‡è¤‡å•é¡Œ
    num_docs = len(docs)
    if num_docs == 0:
        return []

    questions_per_doc = total_questions // num_docs
    extra = total_questions % num_docs

    for idx, doc in enumerate(tqdm(docs, desc="ç”Ÿæˆå‡å•é¡Œä¸­")):
        n = questions_per_doc + (1 if idx < extra else 0)

        for _ in range(n):
            retry = 0
            max_retry = 3  # æœ€å¤šé‡è©¦æ¬¡æ•¸

            while retry < max_retry:
                prompt = (
                    f"è«‹é‡å°ä»¥ä¸‹å…§å®¹ï¼Œæå‡ºä¸€å€‹å¯ä»¥ç›´æ¥å¾å…§æ–‡æ‰¾åˆ°ç­”æ¡ˆçš„æ¸¬è©¦å•é¡Œï¼Œä¸¦åŒ…å«ã€èƒ½å”¯ä¸€æŒ‡æ¶‰ç­”æ¡ˆçš„é—œéµå­—ã€ï¼Œ"
                    f"ä¾‹å¦‚æ•¸å€¼ã€å…¬å¸åã€å¹´ä»½æˆ–å…·é«”äº‹ä»¶ç­‰ï¼Œä½¿å•é¡Œæ˜ç¢ºå°æ‡‰å–®ä¸€ç­”æ¡ˆã€‚\n"
                    f"è¦æ±‚ï¼š\n"
                    f"1ï¸âƒ£ å•é¡Œåƒ…æ ¹æ“šå…§æ–‡ï¼Œä¸åšé¡å¤–æ¨æ¸¬ã€‚\n"
                    f"2ï¸âƒ£ å•é¡Œç°¡çŸ­å…·é«”ã€‚\n"
                    f"3ï¸âƒ£ ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚\n"
                    f"4ï¸âƒ£ åƒ…è¼¸å‡ºå•é¡Œå…§å®¹ï¼Œä¸è¦åŠ ã€Œå•é¡Œï¼šã€æˆ–å…¶ä»–èªªæ˜ã€‚\n\n"
                    f"ç¯„ä¾‹ï¼š\n"
                    f"âŒã€é€™å®¶å…¬å¸ç‡Ÿæ”¶å¦‚ä½•ï¼Ÿã€ï¼ˆä¸å…·å”¯ä¸€æ€§ï¼‰\n"
                    f"âœ…ã€2023 å¹´è©²å…¬å¸ç‡Ÿæ”¶ç‚ºå¤šå°‘ï¼Ÿã€ï¼ˆå…·é«”ã€æ˜ç¢ºï¼‰\n\n"
                    f"å…§æ–‡ï¼š\n{doc['text']}"
                )

                try:
                    response = client.chat.completions.create(
                        model=chat_deployment,
                        messages=[{"role": "user", "content": prompt}],
                        temperature=0.5,
                        max_tokens=100
                    )
                    q = response.choices[0].message.content.strip()
                    a = extract_answer_from_text(doc["text"], q, client, chat_deployment)

                    if q in existing_questions:
                        retry += 1
                        continue  # é‡è¤‡å•é¡Œå°±é‡è©¦
                    else:
                        existing_questions.add(q)
                        questions.append({
                            "document_id": doc["document_id"],
                            "question": q,
                            "answer": a
                        })
                        break  # æˆåŠŸç”Ÿæˆå•é¡Œå°±è·³å‡º retry è¿´åœˆ

                except Exception as e:
                    print(f"âš ï¸ ç”Ÿæˆå‡å•é¡Œå¤±æ•— document_id={doc['document_id']}ï¼ŒåŸå› : {e}")
                    break  # è‹¥æ˜¯ LLM API å¤±æ•—å°±è·³å‡º retry
    return questions

# ----- å¾textæ“·å–answer -----

def extract_answer_from_text(text: str, question: str, client=None, deployment=None) -> str:
    prompt = (
    f"""æ ¹æ“šä»¥ä¸‹å…§å®¹èˆ‡å•é¡Œï¼Œè«‹å¾å…§æ–‡ä¸­æ“·å–**æœ€ç²¾æº–çš„ä¸€æ®µæ–‡å­—**ä½œç‚ºç­”æ¡ˆï¼Œä¸è¦è‡ªè¡Œæ”¹å¯«æˆ–è£œå……ï¼Œ\n
    ç­”æ¡ˆå¿…é ˆæ˜¯åŸæ–‡ä¸­çš„ä¸€æ®µè©±ã€‚\n
    ç­”æ¡ˆæ ¼å¼å¿…é ˆç‚ºï¼š**ã€Œé—œéµè©:æ•¸å­—ã€**ï¼ˆä¾‹å¦‚ï¼šè‚¡åƒ¹è®ŠåŒ–:500ã€ç‡Ÿæ”¶:1200ã€æ·¨åˆ©:80 ç­‰ï¼‰ã€‚\n
    ä¸”å¿…é ˆéµå¾ªä»¥ä¸‹jsonæ ¼å¼:
    
    {text}
        
    ã€å•é¡Œã€‘
    {question}

    ã€è¼¸å‡ºã€‘
    {{
    "answer": "è‚¡åƒ¹è®ŠåŒ–:500"
    }}
    """
    )
    try:
        response = client.chat.completions.create(
            model=deployment,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=100,
            response_format={ "type": "json_object" }
        )
        
        
        json_data = json.loads(response.choices[0].message.content.strip())
        return json_data.get("answer", "")
    except Exception as e:
        print(f"âš ï¸ æ“·å–ç­”æ¡ˆå¤±æ•—ï¼š{e}")
        return ""  # or return None



# ----- embedding å‡½æ•¸ -----

def embed_openai(text: str) -> np.ndarray:
    try:
        response = embedding_ada.embeddings.create(
            input=text,
            model=os.getenv("EMBEDDING_MODEL")
        )
        return np.array(response.data[0].embedding, dtype="float32")
    except Exception as e:
        print(f"embed_openai å‡ºéŒ¯: {e}")
        return np.zeros((1536,), dtype="float32")

def embed_bge(text: str) -> np.ndarray:
    try:
        output = BGE_MODEL.encode(text)
        dense_vecs = output.get("dense_vecs")
        if dense_vecs is None:
            return np.zeros((1024,), dtype="float32")
        return dense_vecs.astype("float32")
    except Exception as e:
        print(f"embed_bge å‡ºéŒ¯: {e}")
        return np.zeros((1024,), dtype="float32")
    
# ----- è³‡æ–™å„²å­˜ -----

def save_question(entry: Dict):
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    cur.execute(
        'INSERT INTO "2500567RAG" (document_id, source_table, question, answer, question_embedding_bge, question_embedding_openai, answer_embedding_bge, answer_embedding_openai, text) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s);',
        (
            entry["document_id"],
            entry["source_table"],
            entry["question"],
            entry["answer"],
            entry["q_emb_bge"].tolist(),
            entry["q_emb_openai"].tolist(),
            entry["a_emb_bge"].tolist(),
            entry["a_emb_openai"].tolist(),
            entry["text"]
        )
    )
    conn.commit()
    conn.close()

def check_and_clear_table_if_needed():
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        cur.execute('SELECT COUNT(*) FROM public."2500567RAG"')
        existing_count = cur.fetchone()[0]
        
        if existing_count > 0:
            print(f"âš ï¸ è³‡æ–™åº«å·²æœ‰ {existing_count} ç­†è¨˜éŒ„")
            resp = input("è«‹é¸æ“‡å‹•ä½œï¼š\n"
                        "y = åˆªé™¤è³‡æ–™ä¸¦é‡æ–°è™•ç†\n"
                        "n = ä¿ç•™è³‡æ–™ä¸¦ç¹¼çºŒè™•ç†\n"
                        "exit = é›¢é–‹ç¨‹å¼\n"
                        "è¼¸å…¥é¸é … (y/n/exit): ").strip().lower()
            
            if resp.lower() == 'y':
                cur.execute('DELETE FROM public."2500567RAG"')
                cur.execute('TRUNCATE TABLE public."2500567RAG" RESTART IDENTITY;')
                conn.commit()
                print("âœ… å·²æ¸…é™¤è³‡æ–™ï¼Œé‡æ–°é–‹å§‹è™•ç†")
            elif resp == 'n':
                print("âœ… ä¿ç•™ç¾æœ‰è³‡æ–™ï¼Œç¹¼çºŒè™•ç†")
            elif resp == 'exit':
                print("ğŸšª å·²å–æ¶ˆè™•ç†ï¼ŒçµæŸç¨‹å¼")
                cur.close()
                conn.close()
                return False
            else:
                print("âš ï¸ ç„¡æ•ˆé¸é …ï¼Œè«‹é‡æ–°åŸ·è¡Œä¸¦è¼¸å…¥ y/n/exit")
                cur.close()
                conn.close()
                return False
        cur.close()
        conn.close()
        return True
    
    except Exception as e:
        print(f"æª¢æŸ¥/æ¸…é™¤è³‡æ–™å¤±æ•—: {e}")
        return False



# ----- ä¸»ç¨‹å¼ -----

def main():
    proceed = check_and_clear_table_if_needed()
    if not proceed:
        return

# =============é–‹å§‹æŠ½å–è³‡æ–™=============
    print("é–‹å§‹ä¾ç…§ source_table é¡å‹æŠ½å–è³‡æ–™...")
    grouped_docs = fetch_documents(limit=1000)

    total_all_questions = 0
    
    for source_table, docs in grouped_docs.items():
        print(f"\nğŸ—‚ï¸ é¡å‹: {source_table}ï¼Œå…± {len(docs)} ç­†")
        print("ğŸ”„ é–‹å§‹ç”Ÿæˆå•é¡Œ...")
        
        questions = generate_questions_for_docs(docs, total_questions=100)
        print(f"âœ… å…±ç‚ºé¡å‹ {source_table} ç”Ÿæˆ {len(questions)} ç­†å•é¡Œ")

        print("ğŸ’¾ ç”Ÿæˆç­”æ¡ˆä¸¦å¯«å…¥è³‡æ–™åº«...")
        for q in tqdm(questions, desc=f"{source_table} - å¯«å…¥ä¸­"):
            doc = next((d for d in docs if d["document_id"] == q["document_id"]), None)
            if doc:
                
                # æ“·å–ç­”æ¡ˆ
                answer = answer = extract_answer_from_text(doc["text"], q["question"], client, os.getenv("AOAI_CHAT_DEPLOYMENT"))
                
                # è¨ˆç®—å‘é‡
                q_emb_bge = embed_bge(q["question"])
                q_emb_openai = embed_openai(q["question"])
                a_emb_bge = embed_bge(doc["text"])
                a_emb_openai = embed_openai(doc["text"])
                
                # å„²å­˜è³‡æ–™
                entry = {
                    "document_id": doc["document_id"],
                    "source_table": doc["source_table"],
                    "question": q["question"],
                    "answer": answer,
                    "q_emb_bge": q_emb_bge,
                    "q_emb_openai": q_emb_openai,
                    "a_emb_bge": a_emb_bge,
                    "a_emb_openai": a_emb_openai,
                    "text": doc["text"]
                }
                save_question(entry)
                
        total_all_questions += len(questions)

    print(f"\nâœ… å…¨éƒ¨é¡å‹å•é¡Œèˆ‡ç­”æ¡ˆç”Ÿæˆå®Œç•¢ï¼ç¸½æ•¸: {total_all_questions} ç­†")



if __name__ == "__main__":
    extract_answer_from_text("æ¸¬è©¦å•é¡Œ", "ä¾†æºåç¨±", client, os.getenv("AOAI_CHAT_DEPLOYMENT"))
    main()
