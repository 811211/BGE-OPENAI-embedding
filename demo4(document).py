# ===========================================
# ğŸ“„ demo2.py
# åŠŸèƒ½ï¼š
# 1. å¾ PostgreSQL è³‡æ–™åº«è®€å–å•é¡Œèˆ‡ç­”æ¡ˆåŠå…¶ embedding
# 2. ä½¿ç”¨ FAISS å»ºç«‹ç´¢å¼•ä¸¦é€²è¡Œæ¨¡å‹æª¢ç´¢è©•ä¼°
# 3. è©•ä¼°æŒ‡æ¨™åŒ…å« Recall@K, Precision@K, Top-K Accuracy, Mean Rank, MRR
# 4. åˆ†æéŒ¯èª¤æ¡ˆä¾‹ä¸¦å„²å­˜è©³ç´°éŒ¯èª¤è³‡è¨Šï¼ˆå«åŸå§‹å•é¡Œã€æ­£ç¢ºç­”æ¡ˆã€æª¢ç´¢çµæœç­‰ï¼‰
# 5. å‘¼å« LLM åˆ†ææ¨¡å‹è¡¨ç¾ï¼Œä¸¦è¼¸å‡ºå®Œæ•´å ±å‘Šèˆ‡çµ±è¨ˆè³‡è¨Š
# ===========================================

import os
import shutil
import tkinter as tk
from tkinter import filedialog
from collections import defaultdict
from typing import List, Dict

import numpy as np
import faiss
import time
import pandas as pd
from tqdm import tqdm
from dotenv import load_dotenv
import psycopg2
from psycopg2.extras import RealDictCursor, execute_values
from collections import defaultdict

from openai import AzureOpenAI
from FlagEmbedding import BGEM3FlagModel

# TODO: ä»¥ä¸‹å€å¡Šå»ºè­°æ•´ç†ç‚ºåŠŸèƒ½å€å¡Šï¼Œä¾‹å¦‚
# - ç’°å¢ƒé…ç½®èˆ‡åˆå§‹åŒ–
# - ç›¸é—œå·¥å…·å‡½æ•¸
# - embedding å‡½æ•¸
# - å»ºç«‹ FAISS ç´¢å¼•
# - è©•ä¼°æŒ‡æ¨™è¨ˆç®—
# - è³‡æ–™å„²å­˜
# - LLM åˆ†æèˆ‡å ±å‘Šç”Ÿæˆ
# - ä¸»ç¨‹å¼

# ----- ç’°å¢ƒé…ç½® -----

load_dotenv()

print("DEBUG: AOAI_KEY =", os.getenv("AOAI_KEY"))
print("DEBUG: AOAI_ENDPOINT =", os.getenv("AOAI_ENDPOINT"))
print("DEBUG: AOAI_API_VERSION =", os.getenv("AOAI_API_VERSION"))
print("DEBUG: AOAI_CHAT_DEPLOYMENT =", os.getenv("AOAI_CHAT_DEPLOYMENT"))

DB_CONFIG = {
    "host": "210.67.12.103",
    "database": "Learning",
    "user": "editor",
    "password": "systex.6214",
    "port": 65432
}

# -----åˆå§‹åŒ– AzureOpenAI client-----

client = AzureOpenAI(
    api_key=os.getenv("AOAI_KEY"),
    azure_endpoint=os.getenv("AOAI_ENDPOINT"),
    api_version=os.getenv("AOAI_API_VERSION")
)

embedding_ada = AzureOpenAI(
    api_key=os.getenv("EMBEDDING_API_KEY"),
    azure_endpoint=os.getenv("EMBEDDING_URL"),
    api_version=os.getenv("AOAI_API_VERSION"),
)

BGE_MODEL = BGEM3FlagModel(
    r"C:\Users\user\.cache\huggingface\hub\models--BAAI--bge-m3\snapshots\5617a9f61b028005a4858fdac845db406aefb181",
    use_fp16=True
)

# ----- å·¥å…·å‡½æ•¸ -----

def check_and_clear_table_if_needed():
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        cur.execute('SELECT COUNT(*) FROM public."2500567RAG"')
        existing_count = cur.fetchone()[0]
        
        if existing_count > 0:
            print(f"âš ï¸ è³‡æ–™åº«å·²æœ‰ {existing_count} ç­†è¨˜éŒ„")
            resp = input("è«‹é¸æ“‡å‹•ä½œï¼š\n"
                        "y = åˆªé™¤è³‡æ–™ä¸¦é‡æ–°è™•ç†\n"
                        "n = ä¿ç•™è³‡æ–™ä¸¦ç¹¼çºŒè™•ç†\n"
                        "exit = é›¢é–‹ç¨‹å¼\n"
                        "è¼¸å…¥é¸é … (y/n/exit): ").strip().lower()
            
            if resp.lower() == 'y':
                cur.execute('DELETE FROM public."2500567RAG"')
                cur.execute('TRUNCATE TABLE public."2500567RAG" RESTART IDENTITY;')
                conn.commit()
                print("âœ… å·²æ¸…é™¤è³‡æ–™ï¼Œé‡æ–°é–‹å§‹è™•ç†")
            elif resp == 'n':
                print("âœ… ä¿ç•™ç¾æœ‰è³‡æ–™ï¼Œç¹¼çºŒè™•ç†")
            elif resp == 'exit':
                print("ğŸšª å·²å–æ¶ˆè™•ç†ï¼ŒçµæŸç¨‹å¼")
                cur.close()
                conn.close()
                return False
            else:
                print("âš ï¸ ç„¡æ•ˆé¸é …ï¼Œè«‹é‡æ–°åŸ·è¡Œä¸¦è¼¸å…¥ y/n/exit")
                cur.close()
                conn.close()
                return False
        cur.close()
        conn.close()
        return True
    
    except Exception as e:
        print(f"æª¢æŸ¥/æ¸…é™¤è³‡æ–™å¤±æ•—: {e}")
        return False
    
def wrap_metrics(metrics: dict) -> dict:
    result = {}
    for k, v in metrics.items():
        if k == "Details":
            result[k] = v  # ä¸åŒ…æˆ list
        else:
            result[k] = [v]  # æ­£å¸¸æŒ‡æ¨™åŒ…æˆ list
    return result

# ----- embedding å‡½æ•¸ -----

def embed_openai(text: str) -> np.ndarray:
    try:
        start = time.time()
        response = embedding_ada.embeddings.create(
            input=text,
            model=os.getenv("EMBEDDING_MODEL")
        )
        duration = time.time() - start
        print(f"[â±ï¸ OpenAI] å–®ç­†è™•ç†æ™‚é–“: {duration:.4f} ç§’")
        return np.array(response.data[0].embedding, dtype="float32")
    except Exception as e:
        print(f"embed_openai å‡ºéŒ¯: {e}")
        return np.zeros((1536,), dtype="float32")

def embed_bge(text: str) -> np.ndarray:
    try:
        start = time.time()
        output = BGE_MODEL.encode(text)
        duration = time.time() - start
        print(f"[â±ï¸ BGE] å–®ç­†è™•ç†æ™‚é–“: {duration:.4f} ç§’")
        dense_vecs = output.get("dense_vecs")
        if dense_vecs is None:
            return np.zeros((1024,), dtype="float32")
        return dense_vecs.astype("float32")
    except Exception as e:
        print(f"embed_bge å‡ºéŒ¯: {e}")
        return np.zeros((1024,), dtype="float32")
    
# ----- å»ºç«‹ FAISS ç´¢å¼• -----

def build_faiss_index(vectors: np.ndarray):
    dim = vectors.shape[1]
    faiss.normalize_L2(vectors)    # COSINE ç›¸ä¼¼åº¦éœ€è¦å…ˆæ­¸ä¸€åŒ– è¦æ”¹L2ç›´æ¥è¨»è§£
    index = faiss.IndexFlatIP(dim) 
    index.add(vectors)
    return index

# ----- è©•ä¼°æŒ‡æ¨™è¨ˆç®— -----

def save_error_cases(details: list, id2text: dict = None, output_dir: str = "report", filename: str = "error_cases.csv"):
    error_cases = [d for d in details if not d.get("hit", False)]
    
    rows = []
    for d in error_cases:
        row = {
            "query_id": d["query_id"],
            "ground_truth": d["ground_truth"],
            "retrieved_ids": d["retrieved_ids"],
            "similarities": d["similarities"],
            "rank": d["rank"],
            "precision": d["precision"],
            "recall": d["recall"],
            "mrr": d["reciprocal_rank"]
        }

        if id2text:
            meta = id2text.get(d["ground_truth"], {})
            row["ground_truth_text"] = meta.get("text", "")
            row["query_text"] = id2text.get(d["query_id"], {}).get("query", "")  
            row["document_id"] = meta.get("document_id", "")
            row["source_table"] = meta.get("source_table", "")
            row["retrieved_texts"] = [
                id2text.get(rid, {}).get("text", "") for rid in d["retrieved_ids"]
            ]


        rows.append(row)

    df = pd.DataFrame(rows)
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, filename)
    df.to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"âŒ éŒ¯èª¤æ¡ˆä¾‹å·²å„²å­˜ï¼š{output_path}")
    
def evaluate_retrieval(index, queries, ground_truth, k=5, id2text=None, index_to_doc_id=None):
    D, I = index.search(queries, k)
    recall_at_k = []
    precision_at_k = []
    top_k_accuracy = []
    mean_rank = []
    reciprocal_ranks = []
    details = []

    for i, retrieved in enumerate(I):
        relevant = ground_truth[i]
        retrieved_list = list(retrieved)
        detail = {
            "query_id": i,
            "ground_truth": relevant,
            "retrieved_ids": [int(x) for x in retrieved_list],
            "similarities": [float(x) for x in D[i]],
            "hit": False,
            "rank": None,
            "recall": 0,
            "precision": 0,
            "mean_rank": k + 1,
            "reciprocal_rank": 0,
            "top1_hit": False
        }
    
        try:
            retrieved_doc_ids = [index_to_doc_id[idx] for idx in retrieved_list]

            detail["retrieved_ids"] = retrieved_doc_ids  # æ›¿æ›åŸæœ¬ FAISS ç´¢å¼•ç·¨è™Ÿç‚ºçœŸå¯¦ document_id

            if ground_truth[i] in retrieved_doc_ids:
                rank = retrieved_doc_ids.index(ground_truth[i])
            else:
               rank = None  # æˆ–è·³é

            recall_at_k.append(1)
            precision_at_k.append(1 / (rank + 1))
            # top_k_accuracy.append(1 if rank == 0 else 0) # Top-1 Accuracy rank == 0 â†’ Top-1 å‘½ä¸­ã€‚
            top_k_accuracy.append(1 if rank < k else 0)    # Top-K Accuracy rank <  k â†’ Top-k å‘½ä¸­
            mean_rank.append(rank + 1)
            reciprocal_ranks.append(1 / (rank + 1))
            detail.update({
                "hit": True,
                "rank": rank + 1,
                "recall": 1,
                "precision": round(1 / (rank + 1), 4),
                "mean_rank": rank + 1,
                "reciprocal_rank": round(1 / (rank + 1), 4),
                "top1_hit": rank == 0
            })
        except ValueError:
            recall_at_k.append(0)
            precision_at_k.append(0)
            top_k_accuracy.append(0)
            mean_rank.append(k + 1)
            reciprocal_ranks.append(0)
            
        details.append(detail)
        
        # ğŸ”¸ å„²å­˜éŒ¯èª¤æ¡ˆä¾‹ CSV
    save_error_cases(details, id2text=id2text)

    return {
        "Recall@K": round(np.mean(recall_at_k), 4),
        "Precision@K": round(np.mean(precision_at_k), 4),
        "Top-K Accuracy": round(np.mean(top_k_accuracy), 4),
        "Mean Rank": round(np.mean(mean_rank), 4),
        "MRR": round(np.mean(reciprocal_ranks), 4),
        "Details": details
    }

# ----- è³‡æ–™å„²å­˜ -----

# def save_evaluation_to_txt(results: dict, run_count: int):
#     filename = f"BGE-OPENAI-embedding test({run_count}).txt"
#     with open(filename, "w", encoding="utf-8") as f:
#         f.write(f"BGE-OPENAI Embedding Evaluation Report (Runs = {run_count})\n")
#         f.write("=" * 60 + "\n\n")
#         for source, models in results.items():
#             f.write(f"ğŸ“„ Source Table: {source}\n")
#             f.write("-" * 60 + "\n")
#             for model_name, metric_data in models.items():
#                 f.write(f"ğŸ”¹ {model_name}:\n")
#                 for metric, values in metric_data.items():
#                     if run_count == 1:
#                         f.write(f"{metric}: {values[0]}\n")
#                     else:
#                         avg = round(sum(values) / len(values), 4)
#                         f.write(f"{metric}: Run1={values[0]}  Run2={values[1]}  Run3={values[2]}  Avg={avg}\n")
#                 f.write("\n")
#     return filename


def prompt_save_as(src_path):
    root = tk.Tk()
    root.withdraw()
    save_path = filedialog.asksaveasfilename(
        title="å¦å­˜çµæœç‚º",
        defaultextension=".txt",
        filetypes=[("Text Files", "*.txt")]
    )
    if save_path:
        shutil.copyfile(src_path, save_path)
        print(f"âœ… æˆåŠŸå¦å­˜ç‚ºï¼š{save_path}")
    else:
        print("âŒ å–æ¶ˆå¦å­˜")
        
def format_details_human_readable(details: List[Dict]) -> str:
    lines = []
    header = f"{'Query':<6}{'GT':<6}{'TopK IDs':<30}{'Rank':<6}{'Top1':<6}{'Recall':<8}{'Prec':<8}{'MRR':<8}"
    lines.append(header)
    lines.append("-" * len(header))
    for d in details:
        line = f"{d['query_id']:<6}{d['ground_truth']:<6}{str(d['retrieved_ids'])[:28]:<30}{str(d['rank'] or '-'):<6}"
        line += f"{'âœ…' if d['top1_hit'] else 'âŒ':<6}{d['recall']:<8}{d['precision']:<8}{d['reciprocal_rank']:<8}"
        lines.append(line)
    return "\n".join(lines)

        
def save_full_report(results_dict, summary_text, run_count):
    import json

    filename = f"å®Œæ•´è©•ä¼°å ±å‘Š_Run{run_count}.txt"
    with open(filename, "w", encoding="utf-8") as f:
        # æ¨™é¡Œ
        f.write(f"[ğŸ“Š] BGE-OPENAI Embedding Evaluation Report (Run {run_count})\n")
        f.write("=" * 80 + "\n\n")

        # æ¯çµ„çµæœ
        for source, models in results_dict.items():
            f.write(f"ğŸ“„ Source Table: {source}\n")
            f.write("-" * 80 + "\n")
            for model_name, metric_data in models.items():
                f.write(f"ğŸ”¹ {model_name} æ¨¡å‹:\n")
                for metric, values in metric_data.items():
                    if run_count == 1:
                        f.write(f"{metric}: {values[0]}\n")
                    else:
                        avg = round(sum(values) / len(values), 4)
                        f.write(f"{metric}: Run1={values[0]}  Run2={values[1]}  Run3={values[2]}  Avg={avg}\n")
                f.write("\n")

        f.write("\n[ğŸ§ ] LLM æ¨¡å‹åˆ†æå ±å‘Š\n")
        f.write("=" * 80 + "\n")
        f.write(summary_text + "\n\n")

        f.write("[ğŸ”] Retrieval è©³ç´°éç¨‹è¨˜éŒ„\n")
        f.write("=" * 80 + "\n")

        for source, models in results_dict.items():
            for model_name, metric_data in models.items():
                if "Details" in metric_data:
                    f.write(f"ğŸ“‚ {source} - {model_name} Retrieval Details:\n")
                    text = format_details_human_readable(metric_data["Details"])
                    f.write(text + "\n\n")               
    return filename

# ----- LLM åˆ†æ -----

def analyze_results_with_llm(results_dict: dict) -> str:
    import json
    text = json.dumps(results_dict, ensure_ascii=False, indent=2)

    analysis_prompt = f"""
ä½ æ˜¯ä¸€å€‹è³‡è¨Šæª¢ç´¢èˆ‡åˆ†æå°ˆå®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æ¨¡å‹å°æ¯”çµæœé€²è¡Œæ·±å…¥åˆ†æã€‚
è«‹ç‰¹åˆ¥é—œæ³¨ Top-1 accuracy èˆ‡ Recall@K æ˜¯å¦å­˜åœ¨æº–ç¢º vs. è¦†è“‹çš„å¹³è¡¡å•é¡Œã€‚

ä½ éœ€è¦æ ¹æ“šä»¥ä¸‹é¢å‘ä¾†ç”¢å‡ºçµæœåˆ†æèˆ‡è¨è«–å®Œæ•´å ±å‘Šï¼š

1. æ•´é«”æº–ç¢ºæ€§æ¯”è¼ƒï¼ˆå“ªå€‹æ¨¡å‹ consistently é ˜å…ˆï¼Ÿï¼‰
2. éŒ¯èª¤æ¡ˆä¾‹å¯èƒ½åŸå› ï¼ˆå“ªäº›æƒ…æ³ä¸‹å…©è€…å·®ç•°å¤§ï¼‰
3. æ€§èƒ½èˆ‡æˆæœ¬æ¯”è¼ƒï¼ˆOpenAI API çš„å»¶é²å’Œè²»ç”¨ vs. æœ¬åœ°éƒ¨ç½²BGEæ¨¡å‹çš„è¨ˆç®—é–‹éŠ·ï¼‰
4. çµè«–ï¼š ç¶œåˆé‡åŒ–æŒ‡æ¨™å’Œè³ªåŒ–åˆ†æï¼Œçµ¦å‡ºæ‡‰ç”¨å»ºè­°ã€‚

è«‹ä»¥æ¸…æ™°æ¢åˆ—æ–¹å¼ç”¢å‡ºåˆ†æå ±å‘Šã€‚ä»¥ä¸‹æ˜¯æŒ‡æ¨™çµæœï¼š
{text}
"""
    response = client.chat.completions.create(
        model=os.getenv("AOAI_CHAT_DEPLOYMENT"),
        messages=[{"role": "user", "content": analysis_prompt}],
        temperature=0.4,
        max_tokens=1000
    )
    return response.choices[0].message.content.strip()


# ----- ä¸»ç¨‹å¼ -----



def main():
    
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor(cursor_factory=RealDictCursor)
    cur.execute('SELECT id, question, answer, source_table, text FROM "2500567RAG" ORDER BY id;')
    rows = cur.fetchall()
    conn.close()

    grouped = defaultdict(list)
    for row in rows:
        grouped[row['source_table']].append(row)

    run_count = 1
    all_results = {}
    
    print(f"ğŸ” é–‹å§‹ä¾ç…§ source_table å…± {len(grouped)} çµ„è³‡æ–™é€²è¡Œè©•ä¼°...\n")
    for source, items in tqdm(grouped.items(), desc="è©•ä¼°ä¸­", unit="çµ„"):
        
        # å³æ™‚è¨ˆç®—å‘é‡
        bge_q = np.array([embed_bge(row["question"]) for row in items], dtype='float32')
        bge_a = np.array([embed_bge(row["text"]) for row in items], dtype='float32')
        openai_q = np.array([embed_openai(row["question"]) for row in items], dtype='float32')
        openai_a = np.array([embed_openai(row["text"]) for row in items], dtype='float32')
        doc_ids = [row["document_id"] for row in items]
        ground_truth = doc_ids

        # å»ºç«‹ FAISS index æ™‚çš„é †åº â†’ doc_id å°ç…§
        index_to_doc_id = {i: doc_id for i, doc_id in enumerate(doc_ids)}
                
        id2text = {
            row["document_id"]: {
                "text": row["text"],
                "query": row["question"],
                "document_id": row["document_id"],
                "source_table": row["source_table"]
            }
            for row in items
        }
 

        result_bge = evaluate_retrieval(build_faiss_index(bge_a), bge_q, ground_truth, id2text=id2text, index_to_doc_id=index_to_doc_id)
        result_openai = evaluate_retrieval(build_faiss_index(openai_a), openai_q, ground_truth, id2text=id2text, index_to_doc_id=index_to_doc_id)

        all_results[source] = {
            "BGE": wrap_metrics(result_bge),
            "OpenAI": wrap_metrics(result_openai)
        }

    # Step 1: ç”¢ç”Ÿåˆ†æå ±å‘Š
    print("\n[ğŸ§ ] LLM åˆ†æå ±å‘Šç”Ÿæˆä¸­...\n")

    # éæ¿¾æ‰ Detailsï¼Œåªä¿ç•™æŒ‡æ¨™æ•¸å€¼
    summary_input = {
        source: {
            model: {k: v for k, v in metrics.items() if k != "Details"}
            for model, metrics in models.items()
        }
        for source, models in all_results.items()
    }

    # ä¸Ÿçµ¦ LLM åˆ†æ
    summary = analyze_results_with_llm(summary_input)
    print(summary)


    # Step 2: æ•´åˆæˆå–®ä¸€å®Œæ•´å ±å‘Š
    full_report_path = save_full_report(all_results, summary, run_count)

    # Step 3: æä¾›ä½¿ç”¨è€…å¦å­˜
    prompt_save_as(full_report_path)


if __name__ == "__main__":
    main()
